# Papers Accepted by EACL2021
- [Papers Accepted by EACL2021](#papers-accepted-by-eacl2021)
  - [一、命名实体识别](#一命名实体识别)
    - [1. Identifying Named Entities as they are Typed](#1-identifying-named-entities-as-they-are-typed)
    - [2.  GLaRA: Graph-based Labeling Rule Augmentation for Weakly Supervised Named Entity Recognition](#2--glara-graph-based-labeling-rule-augmentation-for-weakly-supervised-named-entity-recognition)
    - [3. Document-level Event Entities Extraction with Generative Role-filler Transformers](#3-document-level-event-entities-extraction-with-generative-role-filler-transformers)
    - [4. CHOLAN: A Modular Approach for Neural Entity Linking on Wikipedia and Wikidata](#4-cholan-a-modular-approach-for-neural-entity-linking-on-wikipedia-and-wikidata)
    - [5.  Cross-lingual Entity Alignment with Incidental Supervision](#5--cross-lingual-entity-alignment-with-incidental-supervision)
    - [6. Language Models as Knowledge Bases: On Entity Representations, Storage Capacity, and Paraphrased Queries](#6-language-models-as-knowledge-bases-on-entity-representations-storage-capacity-and-paraphrased-queries)
    - [7. Event-Driven News Stream Clustering using Entity-Aware Contextual Embeddings](#7-event-driven-news-stream-clustering-using-entity-aware-contextual-embeddings)
    - [8. DOCENT: Learning Self-Supervised Entity Representations from Large Document Collections](#8-docent-learning-self-supervised-entity-representations-from-large-document-collections)
    - [9. Enconter: Entity Constrained Progressive Sequence Generation via Insertion-based Transformer](#9-enconter-entity-constrained-progressive-sequence-generation-via-insertion-based-transformer)
    - [10. TDMSci: A Specialized Corpus for Scientific Literature Entity Tagging of Tasks Datasets and Metrics](#10-tdmsci-a-specialized-corpus-for-scientific-literature-entity-tagging-of-tasks-datasets-and-metrics)
    - [11. Boosting Low-Resource Biomedical QA via Entity-Aware Masking Strategies](#11-boosting-low-resource-biomedical-qa-via-entity-aware-masking-strategies)
    - [12. Entity-level Factual Consistency of Abstractive Text Summarization](#12-entity-level-factual-consistency-of-abstractive-text-summarization)
  - [二、关系抽取](#二关系抽取)
    - [1. Multilingual Entity and Relation Extraction Dataset and Model](#1-multilingual-entity-and-relation-extraction-dataset-and-model)
    - [2.  ENPAR:Enhancing Entity and Entity Pair Representations for Joint Entity Relation Extraction](#2--enparenhancing-entity-and-entity-pair-representations-for-joint-entity-relation-extraction)
    - [3. An End-to-end Model for Entity-level Relation Extraction using Multi-Instance Learning](#3-an-end-to-end-model-for-entity-level-relation-extraction-using-multi-instance-learning)
    - [4. Bootstrapping Relation Extractors using Syntactic Search by Examples](#4-bootstrapping-relation-extractors-using-syntactic-search-by-examples)
    - [5. Two Training Strategies for Improving Relation Extraction over Universal Graph](#5-two-training-strategies-for-improving-relation-extraction-over-universal-graph)
    - [6. Learning Relatedness between Types with Prototypes for Relation Extraction](#6-learning-relatedness-between-types-with-prototypes-for-relation-extraction)
    - [7.  Is the Understanding of Explicit Discourse Relations Required in Machine Reading Comprehension?](#7--is-the-understanding-of-explicit-discourse-relations-required-in-machine-reading-comprehension)
    - [8.  “Killing Me” Is Not a Spoiler: Spoiler Detection Model using Graph Neural Networks with Dependency Relation-Aware Attention Mechanism](#8--killing-me-is-not-a-spoiler-spoiler-detection-model-using-graph-neural-networks-with-dependency-relation-aware-attention-mechanism)
  - [三、事件抽取](#三事件抽取)
    - [1. Adapting Event Extractors to Medical Data: Bridging the Covariate Shift](#1-adapting-event-extractors-to-medical-data-bridging-the-covariate-shift)
    - [2. BERT Prescriptions to Avoid Unwanted Headaches: A Comparison of Transformer Architectures for Adverse Drug Event Detection](#2-bert-prescriptions-to-avoid-unwanted-headaches-a-comparison-of-transformer-architectures-for-adverse-drug-event-detection)
    - [3. Probing into the Root: A Dataset for Reason Extraction of Structural Events from Financial Documents](#3-probing-into-the-root-a-dataset-for-reason-extraction-of-structural-events-from-financial-documents)
    - [4. Fine-Grained Event Trigger Detection](#4-fine-grained-event-trigger-detection)
## 一、命名实体识别
### 1. Identifying Named Entities as they are Typed
Ravneet Singh Arora, Chen-Tse Tsai, Daniel Preot¸iuc-Pietro

Bloomberg

**Abstract**

Identifying named entities in written text is an essential component of the text processing pipeline used in applications such as text editors to gain a better understanding of the semantics of the text. However, the typical experimental setup for evaluating Named Entity Recognition (NER) systems is not directly applicable to systems that process text in real time as the text is being typed. Evaluation is performed on a sentence level assuming the end-user is willing to wait until the entire sentence is typed for entities to be identified and further linked to identifiers or co-referenced. We introduce a novel experimental setup for NER systems for applications where decisions about named entity boundaries need to be performed in an online fashion. We study how state-of-the-art methods perform under this setup in multiple languages and propose adaptations to these models to suit this new experimental setup. Experimental results show that the best systems that are evaluated on each token after its typed, reach performance within 1–5 F1 points of systems that are evaluated at the end of the sentence. These show that entity recognition can be performed in this setup and open up the development of other NLP tools in a similar setup.

[[paper]](https://aclanthology.org/2021.eacl-main.84/)

### 2.  GLaRA: Graph-based Labeling Rule Augmentation for Weakly Supervised Named Entity Recognition

**Abstract**

Instead of using expensive manual annotations, researchers have proposed to train named entity recognition (NER) systems using heuristic labeling rules. However, devising labeling rules is challenging because it often requires a considerable amount of manual effort and domain expertise. To alleviate this problem, we propose \textsc{GLaRA}, a graph-based labeling rule augmentation framework, to learn new labeling rules from unlabeled data. We first create a graph with nodes representing candidate rules extracted from unlabeled data. Then, we design a new graph neural network to augment labeling rules by exploring the semantic relations between rules. We finally apply the augmented rules on unlabeled data to generate weak labels and train a NER model using the weakly labeled data. We evaluate our method on three NER datasets and find that we can achieve an average improvement of +20\% F1 score over the best baseline when given a small set of seed rules.

[[paper]](https://arxiv.org/abs/2104.06230)

### 3. Document-level Event Entities Extraction with Generative Role-filler Transformers 
Xinya Du Alexander M. Rush Claire Cardie

Department of Computer Science, Cornell University

**Abstract**

We revisit the classic problem of document-level role-filler entity extraction (REE) for template filling. We argue that sentence-level approaches are ill-suited to the task and introduce a generative transformer-based encoder-decoder framework (GRIT) that is designed to model context at the document level: it can make extraction decisions across sentence boundaries; is implicitly aware of noun phrase coreference structure, and has the capacity to respect cross-role dependencies in the template structure. We evaluate our approach on the MUC-4 dataset, and show that our model performs substantially better than prior work. We also show that our modeling choices contribute to model performance, e.g., by implicitly capturing linguistic knowledge such as recognizing coreferent entity mentions.

[[paper]](https://aclanthology.org/2021.eacl-main.52/)


### 4. CHOLAN: A Modular Approach for Neural Entity Linking on Wikipedia and Wikidata
Manoj Prabhakar Kannan Ravi1, Kuldeep Singh3, Isaiah Onando Mulang’2, Saeedeh Shekarpour4, Johannes Hoffart5, Jens Lehmann2

1 Hasso Plattner Institute, University of Potsdam, Potsdam, Germany

2 Smart Data Analytics, University of Bonn, Bonn, Germany

3 Zerotha Research and Cerence GmbH, Aachen, Germany

4 University of Dayton, Dayton, USA

5 Goldman Sachs, Frankfurt, Germany

**Abstract**

In this paper, we propose CHOLAN, a modular approach to target end-to-end entity linking (EL) over knowledge bases. CHOLAN consists of a pipeline of two transformer-based models integrated sequentially to accomplish the EL task. The first transformer model identifies surface forms (entity mentions) in a given text. For each mention, a second transformer model is employed to classify the target entity among a predefined candidates list. The latter transformer is fed by an enriched context captured from the sentence (i.e. local context), and entity description gained from Wikipedia. Such external contexts have not been used in the state of the art EL approaches. Our empirical study was conducted on two well-known knowledge bases (i.e., Wikidata and Wikipedia). The empirical results suggest that CHOLAN outperforms state-of-the-art approaches on standard datasets such as CoNLL-AIDA, MSNBC, AQUAINT, ACE2004, and T-REx.

[[paper]](https://arxiv.org/abs/2101.09969) last revised 8 Feb 2021

### 5.  Cross-lingual Entity Alignment with Incidental Supervision
Muhao Chen1,2∗, Weijia Shi3∗, Ben Zhou1, Dan Roth1

1 Department of Computer and Information Science, UPenn

2 Viterbi School of Engineering, USC

3 Department of Computer Science, UCLA

**Abstract**

Much research effort has been put to multilingual knowledge graph (KG) embedding methods to address the entity alignment task, which seeks to match entities in different languagespecific KGs that refer to the same real-world object. Such methods are often hindered by the insufficiency of seed alignment provided between KGs. Therefore, we propose an incidentally supervised model, JEANS , which jointly represents multilingual KGs and text corpora in a shared embedding scheme, and seeks to improve entity alignment with incidental supervision signals from text. JEANS first deploys an entity grounding process to combine each KG with the monolingual text corpus. Then, two learning processes are conducted: (i) an embedding learning process to encode the KG and text of each language in one embedding space, and (ii) a selflearning based alignment learning process to iteratively induce the matching of entities and that of lexemes between embeddings. Experiments on benchmark datasets show that JEANS leads to promising improvement on entity alignment with incidental supervision, and significantly outperforms state-of-the-art methods that solely rely on internal information of KGs.

[[paper]](https://arxiv.org/abs/2005.00171) last revised 26 Jan 2021

### 6. Language Models as Knowledge Bases: On Entity Representations, Storage Capacity, and Paraphrased Queries
Benjamin Heinzerling1, 2 and Kentaro Inui2, 1

1RIKEN AIP & 2Tohoku University

**Abstract**

Pretrained language models have been suggested as a possible alternative or complement to structured knowledge bases. However, this emerging LM-as-KB paradigm has so far only been considered in a very limited setting, which only allows handling 21k entities whose single-token name is found in common LM vocabularies. Furthermore, the main benefit of this paradigm, namely querying the KB using a variety of natural language paraphrases, is underexplored so far. Here, we formulate two basic requirements for treating LMs as KBs: (i) the ability to store a large number facts involving a large number of entities and (ii) the ability to query stored facts. We explore three entity representations that allow LMs to represent millions of entities and present a detailed case study on paraphrased querying of world knowledge in LMs, thereby providing a proof-of-concept that language models can indeed serve as knowledge bases.

[[paper]](https://arxiv.org/abs/2008.09036) last revised 21 Apr 2021

### 7. Event-Driven News Stream Clustering using Entity-Aware Contextual Embeddings 
Kailash Karthik Saravanakumar∗2， Miguel Ballesteros1，Muthu Kumar Chandrasekaran1， Kathleen McKeown1,2

1 Amazon AI, USA

2 Department of Computer Science, Columbia University, NY, USA

**Abstract**

We propose a method for online news stream clustering that is a variant of the non-parametric streaming K-means algorithm. Our model uses a combination of sparse and dense document representations, aggregates document-cluster similarity along these multiple representations and makes the clustering decision using a neural classifier. The weighted document-cluster similarity model is learned using a novel adaptation of the triplet loss into a linear classification objective. We show that the use of a suitable fine-tuning objective and external knowledge in pre-trained transformer models yields significant improvements in the effectiveness of contextual embeddings for clustering. Our model achieves a new state-of-the-art on a standard stream clustering dataset of English documents.

[[paper]](https://arxiv.org/abs/2101.11059)

### 8. DOCENT: Learning Self-Supervised Entity Representations from Large Document Collections
Yury Zemlyanskiy∗: U. of Southern California

Sudeep Gandhe: Google Research

Ruining He: Google Research

Bhargav Kanagal: Google Research

Anirudh Ravula: Google Research

Juraj Gottweis: Google Research

Fei Sha†: Google Research

Ilya Eckstein: Google Research

**Abstract**

This paper explores learning rich self-supervised entity representations from large amounts of the associated text. Once pre-trained, these models become applicable to multiple entity-centric tasks such as ranked retrieval, knowledge base completion, question answering, and more. Unlike other methods that harvest self-supervision signals based merely on a local context within a sentence, we radically expand the notion of context to include any available text related to an entity. This enables a new class of powerful, high-capacity representations that can ultimately distill much of the useful information about an entity from multiple text sources, without any human supervision.
We present several training strategies that, unlike prior approaches, learn to jointly predict words and entities -- strategies we compare experimentally on downstream tasks in the TV-Movies domain, such as MovieLens tag prediction from user reviews and natural language movie search. As evidenced by results, our models match or outperform competitive baselines, sometimes with little or no fine-tuning, and can scale to very large corpora.
Finally, we make our datasets and pre-trained models publicly available. This includes Reviews2Movielens (see https://goo.gle/research-docent ), mapping the up to 1B word corpus of Amazon movie reviews (He and McAuley, 2016) to MovieLens tags (Harper and Konstan, 2016), as well as Reddit Movie Suggestions (see this https URL ) with natural language queries and corresponding community recommendations.

[[paper]](https://arxiv.org/abs/2102.13247)

### 9. Enconter: Entity Constrained Progressive Sequence Generation via Insertion-based Transformer 
Lee-Hsun Hsieh, Yang-Yin Lee, Ee-Peng Lim

Singapore Management University, Singapore

**Abstract**

Pretrained using large amount of data, autoregressive language models are able to generate high quality sequences. However, these models do not perform well under hard lexical constraints as they lack fine control of content generation process. Progressive insertion-based transformers can overcome the above limitation and efficiently generate a sequence in parallel given some input tokens as constraint. These transformers however may fail to support hard lexical constraints as their generation process is more likely to terminate prematurely. The paper analyses such early termination problems and proposes the Entity-constrained insertion transformer (ENCONTER), a new insertion transformer that addresses the above pitfall without compromising much generation efficiency. We introduce a new training strategy that considers predefined hard lexical constraints (e.g., entities to be included in the generated sequence). Our experiments show that ENCONTER outperforms other baseline models in several performance metrics rendering it more suitable in practical applications. Our code is available at [this https URL](https://github.com/LARC-CMU-SMU/Enconter)

[[paper]](https://arxiv.org/abs/2103.09548)

### 10. TDMSci: A Specialized Corpus for Scientific Literature Entity Tagging of Tasks Datasets and Metrics 
Yufang Hou, Charles Jochim, Martin Gleize, Francesca Bonin and Debasis Ganguly

IBM Research Europe, Ireland

**Abstract**

Tasks, Datasets and Evaluation Metrics are important concepts for understanding experimental scientific papers. However, most previous work on information extraction for scientific literature mainly focuses on the abstracts only, and does not treat datasets as a separate type of entity (Zadeh and Schumann, 2016; Luan et al., 2018). In this paper, we present a new corpus that contains domain expert annotations for Task (T), Dataset (D), Metric (M) entities on 2,000 sentences extracted from NLP papers. We report experiment results on TDM extraction using a simple data augmentation strategy and apply our tagger to around 30,000 NLP papers from the ACL Anthology. The corpus is made publicly available to the community for fostering research on scientific publication summarization (Erera et al., 2019) and knowledge discovery.

[[paper]](https://arxiv.org/abs/2101.10273)

### 11. Boosting Low-Resource Biomedical QA via Entity-Aware Masking Strategies 
Gabriele Pergola1, Elena Kochkina1,3, Lin Gui1, Maria Liakata1,2,3, Yulan He1

1 University of Warwick, UK 2Queen Mary University of London, UK

3 The Alan Turing Institute, UK

**Abstract**

Biomedical question-answering (QA) has gained increased attention for its capability to provide users with high-quality information from a vast scientific literature. Although an increasing number of biomedical QA datasets has been recently made available, those resources are still rather limited and expensive to produce. Transfer learning via pre-trained language models (LMs) has been shown as a promising approach to leverage existing general-purpose knowledge. However, finetuning these large models can be costly and time consuming, often yielding limited benefits when adapting to specific themes of specialised domains, such as the COVID-19 literature. To bootstrap further their domain adaptation, we propose a simple yet unexplored approach, which we call biomedical entity-aware masking (BEM). We encourage masked language models to learn entity-centric knowledge based on the pivotal entities characterizing the domain at hand, and employ those entities to drive the LM fine-tuning. The resulting strategy is a downstream process applicable to a wide variety of masked LMs, not requiring additional memory or components in the neural architectures. Experimental results show performance on par with state-of-the-art models on several biomedical QA datasets.

[[paper]](https://arxiv.org/abs/2102.08366)

### 12. Entity-level Factual Consistency of Abstractive Text Summarization
Feng Nan1, Ramesh Nallapati1, Zhiguo Wang1, Cicero Nogueira dos Santos1, Henghui Zhu1, Dejiao Zhang1, Kathleen McKeown1,2, Bing Xiang1

1 Amazon Web Services

2 Columbia University

**Abstract**

A key challenge for abstractive summarization is ensuring factual consistency of the generated summary with respect to the original document. For example, state-of-the-art models trained on existing datasets exhibit entity hallucination, generating names of entities that are not present in the source document. We propose a set of new metrics to quantify the entity-level factual consistency of generated summaries and we show that the entity hallucination problem can be alleviated by simply filtering the training data. In addition, we propose a summary-worthy entity classification task to the training process as well as a joint entity and summary generation approach, which yield further improvements in entity level metrics.

[[paper]](https://arxiv.org/abs/2102.09130)

## 二、关系抽取
### 1. Multilingual Entity and Relation Extraction Dataset and Model 
Alessandro Seganti∗2, Klaudia Firlag1, Helena Skowronska*3, Michał Satława1, Piotr Andruszkiewicz1,4

1 Samsung R&D Institute Poland

2 Equinix

3 NextSell, ODC Group

4 Warsaw University of Technology

**Abstract**

We present a novel dataset and model for a multilingual setting to approach the task of Joint Entity and Relation Extraction. The SMiLER dataset consists of 1.1 M annotated sentences, representing 36 relations, and 14 languages. To the best of our knowledge, this is currently both the largest and the most comprehensive dataset of this type. We introduce HERBERTa, a pipeline that combines two independent BERT models: one for sequence classification, and the other for entity tagging. The model achieves micro F1 81.49 for English on this dataset, which is close to the current SOTA on CoNLL, SpERT.

[[paper]](https://aclanthology.org/2021.eacl-main.166/)

### 2.  ENPAR:Enhancing Entity and Entity Pair Representations for Joint Entity Relation Extraction
Yijun Wang1, 2, Changzhi Sun4, Yuanbin Wu3, Hao Zhou4, Lei Li4, and Junchi Yan1, 2

1 Department of Computer Science and Engineering, Shanghai Jiao Tong University

2 MoE Key Lab of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University

3 School of Computer Science and Technology, East China Normal University

4 ByteDance, AI Lab

**Abstract**

Current state-of-the-art systems for joint entity relation extraction (Luan et al., 2019; Wad-den et al., 2019) usually adopt the multi-task learning framework. However, annotations for these additional tasks such as coreference resolution and event extraction are always equally hard (or even harder) to obtain. In this work, we propose a pre-training method ENPAR to improve the joint extraction performance. ENPAR requires only the additional entity annotations that are much easier to collect. Unlike most existing works that only consider incorporating entity information into the sentence encoder, we further utilize the entity pair information. Specifically, we devise four novel objectives,i.e., masked entity typing, masked entity prediction, adversarial context discrimination, and permutation prediction, to pre-train an entity encoder and an entity pair encoder. Comprehensive experiments show that the proposed pre-training method achieves significant improvement over BERT on ACE05, SciERC, and NYT, and outperforms current state-of-the-art on ACE05.

[[paper]](https://aclanthology.org/2021.eacl-main.251/)

### 3. An End-to-end Model for Entity-level Relation Extraction using Multi-Instance Learning 
Markus Eberts, Adrian Ulges

RheinMain University of Applied Sciences, Wiesbaden, Germany

**Abstract**

We present a joint model for entity-level relation extraction from documents. In contrast to other approaches - which focus on local intra-sentence mention pairs and thus require annotations on mention level - our model operates on entity level. To do so, a multi-task approach is followed that builds upon coreference resolution and gathers relevant signals via multi-instance learning with multi-level representations combining global entity and local mention information. We achieve state-of-the-art relation extraction results on the DocRED dataset and report the first entity-level end-to-end relation extraction results for future reference. Finally, our experimental results suggest that a joint approach is on par with task-specific learning, though more efficient due to shared parameters and training steps.

[[paper]](https://arxiv.org/abs/2102.05980)

### 4. Bootstrapping Relation Extractors using Syntactic Search by Examples 
Matan Eyal1, Asaf Amrami1,2, Hillel Taub-Tabib1, Yoav Goldberg1, 2

1 Allen Institute for AI, Tel Aviv, Israel

2 Bar Ilan University, Ramat-Gan, Israel

**Abstract**

The advent of neural-networks in NLP brought with it substantial improvements in supervised relation extraction. However, obtaining a sufficient quantity of training data remains a key challenge. In this work we propose a process for bootstrapping training datasets which can be performed quickly by non-NLP-experts. We take advantage of search engines over syntactic-graphs (Such as Shlain et al. (2020)) which expose a friendly by-example syntax. We use these to obtain positive examples by searching for sentences that are syntactically similar to user input examples. We apply this technique to relations from TACRED and DocRED and show that the resulting models are competitive with models trained on manually annotated data and on data obtained from distant supervision. The models also outperform models trained using NLG data augmentation techniques. Extending the search-based approach with the NLG method further improves the results.

[[paper]](https://arxiv.org/abs/2102.05007)

### 5. Two Training Strategies for Improving Relation Extraction over Universal Graph 
Qin Dai1, Naoya Inoue2, Ryo Takahashi1,3, Kentaro Inui1,3

1 Tohoku University, Japan

2 Stony Brook University, USA

3 RIKEN Center for Advanced Intelligence Project, Japan

**Abstract**

This paper explores how the Distantly Supervised Relation Extraction (DS-RE) can benefit from the use of a Universal Graph (UG), the combination of a Knowledge Graph (KG) and a large-scale text collection. A straightforward extension of a current state-of-the-art neural model for DS-RE with a UG may lead to degradation in performance. We first report that this degradation is associated with the difficulty in learning a UG and then propose two training strategies: (1) Path Type Adaptive Pretraining, which sequentially trains the model with different types of UG paths so as to prevent the reliance on a single type of UG path; and (2) Complexity Ranking Guided Attention mechanism, which restricts the attention span according to the complexity of a UG path so as to force the model to extract features not only from simple UG paths but also from complex ones. Experimental results on both biomedical and NYT10 datasets prove the robustness of our methods and achieve a new state-of-the-art result on the NYT10 dataset. The code and datasets used in this paper are available at [this https URL](https://github.com/baodaiqin/UGDSRE).

[[paper]](https://arxiv.org/abs/2102.06540)  last revised 6 May 2021

### 6. Learning Relatedness between Types with Prototypes for Relation Extraction
Lisheng Fu∗ Ralph Grishman

Computer Science Department, New York University, New York, NY 10003, USA

**Abstract**

Relation schemas are often pre-defined for each relation dataset. Relation types can be related from different datasets and have overlapping semantics. We hypothesize we can combine these datasets according to the semantic relatedness between the relation types to overcome the problem of lack of training data. It is often easy to discover the connection between relation types based on relation names or annotation guides, but hard to measure the exact similarity and take advantage of the connection between the relation types from different datasets. We propose to use prototypical examples to represent each relation type and use these examples to augment related types from a different dataset. We obtain further improvement (ACE05) with this type augmentation over a strong baseline which uses multi-task learning between datasets to obtain better feature representation for relations. We make our implementation publicly available: https://github.com/fufrank5/relatedness

[[paper]](https://aclanthology.org/2021.eacl-main.172/)

### 7.  Is the Understanding of Explicit Discourse Relations Required in Machine Reading Comprehension?
Yulong Wu, Viktor Schlegel and Riza Batista-Navarro

Department of Computer Science, University of Manchester, Manchester, United Kingdom

**Abstract**

An in-depth analysis of the level of language understanding required by existing Machine Reading Comprehension (MRC) benchmarks can provide insight into the reading capabilities of machines. In this paper, we propose an ablation-based methodology to assess the extent to which MRC datasets evaluate the understanding of explicit discourse relations. We define seven MRC skills which require the understanding of different discourse relations. We then introduce ablation methods that verify whether these skills are required to succeed on a dataset. By observing the drop in performance of neural MRC models evaluated on the original and the modified dataset, we can measure to what degree the dataset requires these skills, in order to be understood correctly. Experiments on three large-scale datasets with the BERT-base and ALBERT-xxlarge model show that the relative changes for all skills are small (less than 6%). These results imply that most of the answered questions in the examined datasets do not require understanding the discourse structure of the text. To specifically probe for natural language understanding, there is a need to design more challenging benchmarks that can correctly evaluate the intended skills.

[[paper]](https://aclanthology.org/2021.eacl-main.311/)

### 8.  “Killing Me” Is Not a Spoiler: Spoiler Detection Model using Graph Neural Networks with Dependency Relation-Aware Attention Mechanism
Buru Chang†, Inggeol Lee‡ 

Hyunjae Kim‡ Jaewoo Kang‡∗

Hyperconnect† Korea University‡

**Abstract**

Several machine learning-based spoiler detection models have been proposed recently to protect users from spoilers on review websites. Although dependency relations between context words are important for detecting spoilers, current attention-based spoiler detection models are insufficient for utilizing dependency relations. To address this problem, we propose a new spoiler detection model called SDGNN that is based on syntax-aware graph neural networks. In the experiments on two real-world benchmark datasets, we show that our SDGNN outperforms the existing spoiler detection models.

[[paper]](https://arxiv.org/abs/2101.05972)

## 三、事件抽取
### 1. Adapting Event Extractors to Medical Data: Bridging the Covariate Shift 
Aakanksha Naik1， Jill Lehman2，Carolyn Rose1

1 Language Technologies Institute, Carnegie Mellon University

2 Human-Computer Interaction Institute, Carnegie Mellon University

**Abstract**

We tackle the task of adapting event extractors to new domains without labeled data, by aligning the marginal distributions of source and target domains. As a testbed, we create two new event extraction datasets using English texts from two medical domains: (i) clinical notes, and (ii) doctor-patient conversations. We test the efficacy of three marginal alignment techniques: (i) adversarial domain adaptation (ADA), (ii) domain adaptive fine-tuning (DAFT), and (iii) a novel instance weighting technique based on language model likelihood scores (LIW). LIW and DAFT improve over a no-transfer BERT baseline on both domains, but ADA only improves on clinical notes. Deeper analysis of performance under different types of shifts (e.g., lexical shift, semantic shift) reveals interesting variations among models. Our best-performing models reach F1 scores of 70.0 and 72.9 on notes and conversations respectively, using no labeled data from target domains.

[[paper]](https://arxiv.org/abs/2008.09266)

### 2. BERT Prescriptions to Avoid Unwanted Headaches: A Comparison of Transformer Architectures for Adverse Drug Event Detection 
Beatrice Portelli1， Edoardo Lenzi1 Emmanuele Chersoni2，Giuseppe Serra1， Enrico Santus3

1 AILAB UniUd - University of Udine, Italy

2 The Hong Kong Polytechnic University

3 Decision Science and Advanced Analytics for MAPV & RA, Bayer

**Abstract**

Pretrained transformer-based models, such as BERT and its variants, have become a common choice to obtain state-of-the-art performances in NLP tasks. In the identification of Adverse Drug Events (ADE) from social media texts, for example, BERT architectures rank first in the leaderboard. However, a systematic comparison between these models has not yet been done. In this paper, we aim at shedding light on the differences between their performance analyzing the results of 12 models, tested on two standard benchmarks. SpanBERT and PubMedBERT emerged as the best models in our evaluation: this result clearly shows that span-based pretraining gives a decisive advantage in the precise recognition of ADEs, and that in-domain language pretraining is particularly useful when the transformer model is trained just on biomedical text from scratch.

[[paper]](https://aclanthology.org/2021.eacl-main.149/)

### 3. Probing into the Root: A Dataset for Reason Extraction of Structural Events from Financial Documents 
Pei Chen1∗, Kang Liu2,3, Yubo Chen2,3, Taifeng Wang4, and Jun Zhao2,3

1 Texas A&M University, College Station, TX

2 Institute of Automation, Chinese Academy of Sciences, Beijing, China

3 University of Chinese Academy of Sciences, Beijing, China

4 Ant Group, Hangzhou, China

**Abstract**

This paper proposes a new task regarding event reason extraction from document-level texts. Unlike the previous causality detection task, we do not assign target events in the text, but only provide structural event descriptions, and such settings accord more with practice scenarios. Moreover, we annotate a large dataset FinReason for evaluation, which provides Reasons annotation for Financial events in company announcements. This task is challenging because the cases of multiple-events, multiple-reasons, and implicit-reasons are included. In total, FinReason contains 8,794 documents, 12,861 financial events and 11,006 reason spans. We also provide the performance of existing canonical methods in event extraction and machine reading comprehension on this task. The results show a 7 percentage point F1 score gap between the best model and human performance, and existing methods are far from resolving this problem.

[[paper]](https://aclanthology.org/2021.eacl-main.175/)

### 4. Fine-Grained Event Trigger Detection 
Duong Minh Le: VinAI Research, Vietnam

Thien Huu Nguyen∗: Department of Computer and Information Science, University of Oregon, Eugene, OR 97403, USA

**Abstract**

Most of the previous work on Event Detection (ED) has only considered the datasets with a small number of event types (i.e., up to 38 types). In this work, we present the first study on fine-grained ED (FED) where the evaluation dataset involves much more fine-grained event types (i.e., 449 types). We propose a novel method to transform the Semcor dataset for Word Sense Disambiguation into a large and high-quality dataset for FED. Extensive evaluation of the current ED methods is conducted to demonstrate the challenges of the generated datasets for FED, calling for more research effort in this area.

[[paper]](https://aclanthology.org/2021.eacl-main.237/)