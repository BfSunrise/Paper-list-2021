# Papers Accepted by ACL 2021
- [Papers Accepted by ACL 2021](#papers-accepted-by-acl-2021)
  - [一、命名实体识别](#一命名实体识别)
    - [嵌套&非连续NER](#嵌套非连续ner)
      - [1. A Span-Based Model for Joint Overlapped and Discontinuous Named Entity Recognition](#1-a-span-based-model-for-joint-overlapped-and-discontinuous-named-entity-recognition)
      - [2. Locate and Label: A Two-stage Identifier for Nested Named Entity Recognition](#2-locate-and-label-a-two-stage-identifier-for-nested-named-entity-recognition)
      - [3. Nested Named Entity Recognition via Explicitly Excluding the Influence of the Best Path](#3-nested-named-entity-recognition-via-explicitly-excluding-the-influence-of-the-best-path)
      - [4. Discontinuous Named Entity Recognition as Maximal Clique Discovery](#4-discontinuous-named-entity-recognition-as-maximal-clique-discovery)
      - [5. A Unified Generative Framework for Various NER Subtasks](#5-a-unified-generative-framework-for-various-ner-subtasks)
    - [少样本NER](#少样本ner)
      - [1. Subsequence Based Deep Active Learning for Named Entity Recognition](#1-subsequence-based-deep-active-learning-for-named-entity-recognition)
      - [2. Few-NERD: A Few-shot Named Entity Recognition Dataset](#2-few-nerd-a-few-shot-named-entity-recognition-dataset)
      - [3. Named Entity Recognition with Small Strongly Labeled and Large Weakly Labeled Data](#3-named-entity-recognition-with-small-strongly-labeled-and-large-weakly-labeled-data)
      - [4. Weakly Supervised Named Entity Tagging with Learnable Logical Rules](#4-weakly-supervised-named-entity-tagging-with-learnable-logical-rules)
      - [5.Leveraging Type Descriptions for Zero-shot Named Entity Recognition and Classification](#5leveraging-type-descriptions-for-zero-shot-named-entity-recognition-and-classification)
      - [6. Learning from Miscellaneous Other-Class Words for Few-shot Named Entity Recognition](#6-learning-from-miscellaneous-other-class-words-for-few-shot-named-entity-recognition)
    - [中文&多模NER](#中文多模ner)
      - [1. MECT: Multi-Metadata Embedding based Cross-Transformer for Chinese Named Entity Recognition](#1-mect-multi-metadata-embedding-based-cross-transformer-for-chinese-named-entity-recognition)
      - [2. A Large-Scale Chinese Multimodal NER Dataset with Speech Clues](#2-a-large-scale-chinese-multimodal-ner-dataset-with-speech-clues)
    - [实体标准化](#实体标准化)
      - [1. An End-to-End Progressive Multi-Task Learning Framework for Medical Named Entity Recognition and Normalization](#1-an-end-to-end-progressive-multi-task-learning-framework-for-medical-named-entity-recognition-and-normalization)
      - [2. A Neural Transition-based Joint Model for Disease Named Entity Recognition and Normalization](#2-a-neural-transition-based-joint-model-for-disease-named-entity-recognition-and-normalization)
    - [实体分类](#实体分类)
      - [1. Modeling Fine-Grained Entity Types with Box Embeddings](#1-modeling-fine-grained-entity-types-with-box-embeddings)
      - [2. Ultra-Fine Entity Typing with Weak Supervision from a Masked Language Model](#2-ultra-fine-entity-typing-with-weak-supervision-from-a-masked-language-model)
    - [其他](#其他)
      - [1. SpanNER: Named Entity Re-/Recognition as Span Prediction](#1-spanner-named-entity-re-recognition-as-span-prediction)
      - [2. Improving Named Entity Recognition by External Context Retrieving and Cooperative Learning](#2-improving-named-entity-recognition-by-external-context-retrieving-and-cooperative-learning)
      - [3. Modularized Interaction Network for Named Entity Recognition](#3-modularized-interaction-network-for-named-entity-recognition)
      - [4. BERTifying the Hidden Markov Model for Multi-Source Weakly Supervised Named Entity Recognition](#4-bertifying-the-hidden-markov-model-for-multi-source-weakly-supervised-named-entity-recognition)
      - [5. De-biasing Distantly Supervised Named Entity Recognition via Causal Intervention](#5-de-biasing-distantly-supervised-named-entity-recognition-via-causal-intervention)
      - [6. Crowdsourcing Learning as Domain Adaptation: A Case Study on Named Entity Recognition](#6-crowdsourcing-learning-as-domain-adaptation-a-case-study-on-named-entity-recognition)
      - [7. LNN-EL: A Neuro-Symbolic Approach to Short-text Entity Linking](#7-lnn-el-a-neuro-symbolic-approach-to-short-text-entity-linking)
  - [二、关系抽取](#二关系抽取)
    - [远程监督](#远程监督)
      - [1. CIL: Contrastive Instance Learning Framework for Distantly Supervised Relation Extraction](#1-cil-contrastive-instance-learning-framework-for-distantly-supervised-relation-extraction)
      - [2. How Knowledge Graph and Attention Help? A Qualitative Analysis into Bag-level Relation Extraction](#2-how-knowledge-graph-and-attention-help-a-qualitative-analysis-into-bag-level-relation-extraction)
      - [2. SENT: Sentence-level Distant Relation Extraction via Negative Training](#2-sent-sentence-level-distant-relation-extraction-via-negative-training)
      - [3. Revisiting the Negative Data of Distantly Supervised Relation Extraction](#3-revisiting-the-negative-data-of-distantly-supervised-relation-extraction)
    - [联合抽取](#联合抽取)
      - [1. Joint Biomedical Entity and Relation Extraction with Knowledge-Enhanced Collective Inference](#1-joint-biomedical-entity-and-relation-extraction-with-knowledge-enhanced-collective-inference)
      - [2. UniRE: A Unified Label Space for Entity Relation Extraction](#2-unire-a-unified-label-space-for-entity-relation-extraction)
      - [3. PRGC: Potential Relation and Global Correspondence Based Joint Relational Triple Extraction](#3-prgc-potential-relation-and-global-correspondence-based-joint-relational-triple-extraction)
      - [4. Dependency-driven Relation Extraction with Attentive Graph Convolutional Networks](#4-dependency-driven-relation-extraction-with-attentive-graph-convolutional-networks)
    - [开放抽取](#开放抽取)
      - [1. CoRI: Collective Relation Integration with Data Augmentation for Open Information Extraction](#1-cori-collective-relation-integration-with-data-augmentation-for-open-information-extraction)
      - [2. Element Intervention for Open Relation Extraction](#2-element-intervention-for-open-relation-extraction)
    - [事件关系抽取](#事件关系抽取)
      - [1. From Discourse to Narrative: Knowledge Projection for Event Relation Extraction](#1-from-discourse-to-narrative-knowledge-projection-for-event-relation-extraction)
    - [其他](#其他-1)
      - [1. Refining Sample Embeddings with Relation Prototypes to Enhance Continual Relation Extraction](#1-refining-sample-embeddings-with-relation-prototypes-to-enhance-continual-relation-extraction)
    - [三、事件抽取](#三事件抽取)
      - [1. Capturing Event Argument Interaction via A Bi-Directional Entity-Level Recurrent Decoder](#1-capturing-event-argument-interaction-via-a-bi-directional-entity-level-recurrent-decoder)
      - [2. Verb Knowledge Injection for Multilingual Event Processing](#2-verb-knowledge-injection-for-multilingual-event-processing)
      - [3. OntoED: Low-resource Event Detection with Ontology Embedding](#3-ontoed-low-resource-event-detection-with-ontology-embedding)
      - [4. Document-level Event Extraction via Heterogeneous Graph-based Interaction Model with a Tracker](#4-document-level-event-extraction-via-heterogeneous-graph-based-interaction-model-with-a-tracker)
      - [5. LearnDA: Learnable Knowledge-Guided Data Augmentation for Event Causality Identification](#5-learnda-learnable-knowledge-guided-data-augmentation-for-event-causality-identification)
      - [6. MLBiNet: A Cross-Sentence Collective Event Detection Network](#6-mlbinet-a-cross-sentence-collective-event-detection-network)
      - [7. Unleash GPT-2 Power for Event Detection](#7-unleash-gpt-2-power-for-event-detection)
      - [8. Document-Level Event Argument Extraction via Optimal Transport](#8-document-level-event-argument-extraction-via-optimal-transport)
      - [9. Document-level Event Extraction via Parallel Prediction Networks](#9-document-level-event-extraction-via-parallel-prediction-networks)
      - [10. Trigger is Not Sufficient: Exploiting Frame-aware Knowledge for Implicit Event Argument Extraction](#10-trigger-is-not-sufficient-exploiting-frame-aware-knowledge-for-implicit-event-argument-extraction)
      - [11. The Possible, the Plausible, and the Desirable: Event-Based Modality Detection for Language Processing](#11-the-possible-the-plausible-and-the-desirable-event-based-modality-detection-for-language-processing)
      - [12. Text2Event: Controllable Sequence-to-Structure Generation for End-to-end Event Extraction](#12-text2event-controllable-sequence-to-structure-generation-for-end-to-end-event-extraction)
    - [四、信息抽取预训练](#四信息抽取预训练)
      - [1. ERICA: Improving Entity and Relation Understanding for Pre-trained Language Models via Contrastive Learning](#1-erica-improving-entity-and-relation-understanding-for-pre-trained-language-models-via-contrastive-learning)
      - [2.CLEVE: Contrastive Pre-training for Event Extraction](#2cleve-contrastive-pre-training-for-event-extraction)
## 一、命名实体识别
### 嵌套&非连续NER
#### 1. A Span-Based Model for Joint Overlapped and Discontinuous Named Entity Recognition
Fei Li1, Zhichao Lin2, Meishan Zhang2, Donghong Ji1∗

1 Department of Key Laboratory of Aerospace Information Security and Trusted Computing, Ministry of Education, School of Cyber Science and Engineering, Wuhan University, China

2 School of New Media and Communication, Tianjin University, China

**Abstract**

Research on overlapped and discontinuous named entity recognition (NER) has received increasing attention. The majority of previous work focuses on either overlapped or discontinuous entities. In this paper, we propose a novel span-based model that can recognize both overlapped and discontinuous entities jointly. The model includes two major steps. First, entity fragments are recognized by traversing over all possible text spans, thus, overlapped entities can be recognized. Second, we perform relation classification to judge whether a given pair of entity fragments to be overlapping or succession. In this way, we can recognize not only discontinuous entities, and meanwhile doubly check the overlapped entities. As a whole, our model can be regarded as a relation extraction paradigm essentially. Experimental results on multiple benchmark datasets (i.e., CLEF, GENIA and ACE05) show that our model is highly competitive for overlapped and discontinuous NER.

[[paper]](https://arxiv.org/abs/2106.14373) Submitted on 28 Jun 2021

#### 2. Locate and Label: A Two-stage Identifier for Nested Named Entity Recognition
Yongliang Shen1, Xinyin Ma1, Zeqi Tan1, Shuai Zhang1, Wen Wang2, Weiming Lu1

1 College of Computer Science and Technology, Zhejiang University

2 University of Science and Technology of China

**Abstract**

Named entity recognition (NER) is a well-studied task in natural language processing. Traditional NER research only deals with flat entities and ignores nested entities. The span-based methods treat entity recognition as a span classification task. Although these methods have the innate ability to handle nested NER, they suffer from high computational cost, ignorance of boundary information, under-utilization of the spans that partially match with entities, and difficulties in long entity recognition. To tackle these issues, we propose a two-stage entity identifier. First we generate span proposals by filtering and boundary regression on the seed spans to locate the entities, and then label the boundary-adjusted span proposals with the corresponding categories. Our method effectively utilizes the boundary information of entities and partially matched spans during training. Through boundary regression, entities of any length can be covered theoretically, which improves the ability to recognize long entities. In addition, many low-quality seed spans are filtered out in the first stage, which reduces the time complexity of inference. Experiments on nested NER datasets demonstrate that our proposed method outperforms previous state-of-the-art models.

[[paper]](https://arxiv.org/abs/2105.06804) last revised 13 Jul 2021

#### 3. Nested Named Entity Recognition via Explicitly Excluding the Influence of the Best Path
Yiran Wang1∗, Hiroyuki Shindo2, Yuji Matsumoto3, Taro Watanabe2

1 National Institute of Information and Communications Technology (NICT), Kyoto, Japan

2 Nara Institute of Science and Technology (NAIST), Nara, Japan

3 RIKEN Center for Advanced Intelligence Project (AIP), Tokyo, Japan 

**Abstract**

This paper presents a novel method for nested named entity recognition. As a layered method, our method extends the prior second-best path recognition method by explicitly excluding the influence of the best path. Our method maintains a set of hidden states at each time step and selectively leverages them to build a different potential function for recognition at each level. In addition, we demonstrate that recognizing innermost entities first results in better performance than the conventional outermost entities first scheme. We provide extensive experimental results on ACE2004, ACE2005, and GENIA datasets to show the effectiveness and efficiency of our proposed
method.

[[paper]](https://aclanthology.org/2021.acl-long.275.pdf)

#### 4. Discontinuous Named Entity Recognition as Maximal Clique Discovery
Yucheng Wang∗, Bowen Yu∗, Hongsong Zhu, Tingwen Liu†, Nan Yu, Limin Sun

Institute of Information Engineering, Chinese Academy of Sciences

School of Cyber Security, University of Chinese Academy of Sciences

**Abstract**

Named entity recognition (NER) remains challenging when entity mentions can be discontinuous. Existing methods break the recognition process into several sequential steps. In training, they predict conditioned on the golden intermediate results, while at inference relying on the model output of the previous steps, which introduces exposure bias. To solve this problem, we first construct a segment graph for each sentence, in which each node denotes a segment (a continuous entity on its own, or a part of discontinuous entities), and an edge links two nodes that belong to the same entity. The nodes and edges can be generated respectively in one stage with a grid tagging scheme and learned jointly using a novel architecture named Mac. Then discontinuous NER can be reformulated as a non-parametric process of discovering maximal cliques in the graph and concatenating the spans in each clique. Experiments on three benchmarks show that our method outperforms the state-of-the-art (SOTA) results, with up to 3.5 percentage points improvement on F1, and achieves 5x speedup over the SOTA model.

[[paper]](https://arxiv.org/abs/2106.00218) Submitted on 1 Jun 2021

#### 5. A Unified Generative Framework for Various NER Subtasks
Hang Yan1, Tao Gui2, Junqi Dai1, Qipeng Guo1, Zheng Zhang3, Xipeng Qiu1,4∗

1 Shanghai Key Laboratory of Intelligent Information Processing, Fudan University

1 School of Computer Science, Fudan University

2 Institute of Modern Languages and Linguistics, Fudan University

3 New York University

4 Pazhou Lab, Guangzhou, China

**Abstract**

Named Entity Recognition (NER) is the task of identifying spans that represent entities in sentences. Whether the entity spans are nested or discontinuous, the NER task can be categorized into the flat NER, nested NER, and discontinuous NER subtasks. These subtasks have been mainly solved by the token-level sequence labelling or span-level classification. However, these solutions can hardly tackle the three kinds of NER subtasks concurrently. To that end, we propose to formulate the NER subtasks as an entity span sequence generation task, which can be solved by a unified sequence-to-sequence (Seq2Seq) framework. Based on our unified framework, we can leverage the pre-trained Seq2Seq model to solve all three kinds of NER subtasks without the special design of the tagging schema or ways to enumerate spans. We exploit three types of entity representations to linearize entities into a sequence. Our proposed framework is easy-to-implement and achieves state-of-the-art (SoTA) or near SoTA performance on eight English NER datasets, including two flat NER datasets, three nested NER datasets, and three discontinuous NER datasets.

[[paper]](https://arxiv.org/abs/2106.01223) Submitted on 2 Jun 2021

### 少样本NER

#### 1. Subsequence Based Deep Active Learning for Named Entity Recognition
Puria Radmard1,2,3, Yassir Fathullah2, Aldo Lipani1,3

1 University College London

2 University of Cambridge

3 Vector AI

**Abstract**

Active Learning (AL) has been successfullyapplied to Deep Learning in order to drasti-cally reduce the amount of data required toachieve high performance. Previous workshave shown that lightweight architectures forNamed Entity Recognition (NER) can achieveoptimal performance with only 25% of theoriginal training data. However, these meth-ods do not exploit the sequential nature oflanguage and the heterogeneity of uncertaintywithin each instance, requiring the labelling ofwhole sentences. Additionally, this standardmethod requires that the annotator has accessto the full sentence when labelling. In thiswork, we overcome these limitations by allow-ing the AL algorithm to query subsequenceswithin sentences, and propagate their labels toother sentences. We achieve highly efﬁcientresults on OntoNotes 5.0, only requiring 13%of the original training data, and CoNLL 2003,requiring only 27%. This is an improvement of39% and 37% compared to querying full sen-tences.

[[paper]](https://www.researchgate.net/publication/351885762_Subsequence_Based_Deep_Active_Learning_for_Named_Entity_Recognition) May 2021

#### 2. Few-NERD: A Few-shot Named Entity Recognition Dataset
Ning Ding1,3∗, Guangwei Xu2∗, Yulin Chen3∗, Xiaobin Wang2, Xu Han1, Pengjun Xie2, Hai-Tao Zheng3†, Zhiyuan Liu1†

1 Department of Computer Science and Technology, Tsinghua University

2 Alibaba Group

3 Shenzhen International Graduate School, Tsinghua University

**Abstract**

Recently, considerable literature has grown up around the theme of few-shot named entity recognition (NER), but little published benchmark data specifically focused on the practical and challenging task. Current approaches collect existing supervised NER datasets and re-organize them to the few-shot setting for empirical study. These strategies conventionally aim to recognize coarse-grained entity types with few examples, while in practice, most unseen entity types are fine-grained. In this paper, we present Few-NERD, a large-scale human-annotated few-shot NER dataset with a hierarchy of 8 coarse-grained and 66 fine-grained entity types. Few-NERD consists of 188,238 sentences from Wikipedia, 4,601,160 words are included and each is annotated as context or a part of a two-level entity type. To the best of our knowledge, this is the first few-shot NER dataset and the largest human-crafted NER dataset. We construct benchmark tasks with different emphases to comprehensively assess the generalization capability of models. Extensive empirical results and analysis show that Few-NERD is challenging and the problem requires further research. We make Few-NERD public at [this https URL](https://ningding97.github.io/fewnerd/).

[[paper]](https://arxiv.org/abs/2105.07464v5) last revised 20 Jun 2021

#### 3. Named Entity Recognition with Small Strongly Labeled and Large Weakly Labeled Data
Haoming Jiang1, Danqing Zhang2, Tianyu Cao2, Bing Yin2, Tuo Zhao1

1 Georgia Institute of Technology, Atlanta, GA, USA

2 Amazon.com Inc, Palo Alto, CA, USA

**Abstract**

Weak supervision has shown promising results in many natural language processing tasks, such as Named Entity Recognition (NER). Existing work mainly focuses on learning deep NER models only with weak supervision, i.e., without any human annotation, and shows that by merely using weakly labeled data, one can achieve good performance, though still underperforms fully supervised NER with manually/strongly labeled data. In this paper, we consider a more practical scenario, where we have both a small amount of strongly labeled data and a large amount of weakly labeled data. Unfortunately, we observe that weakly labeled data does not necessarily improve, or even deteriorate the model performance (due to the extensive noise in the weak labels) when we train deep NER models over a simple or weighted combination of the strongly labeled and weakly labeled data. To address this issue, we propose a new multi-stage computational framework -- NEEDLE with three essential ingredients: (1) weak label completion, (2) noise-aware loss function, and (3) final fine-tuning over the strongly labeled data. Through experiments on E-commerce query NER and Biomedical NER, we demonstrate that NEEDLE can effectively suppress the noise of the weak labels and outperforms existing methods. In particular, we achieve new SOTA F1-scores on 3 Biomedical NER datasets: BC5CDR-chem 93.74, BC5CDR-disease 90.69, NCBI-disease 92.28.

[[paper]](https://arxiv.org/abs/2106.08977) last revised 1 Aug 2021 

#### 4. Weakly Supervised Named Entity Tagging with Learnable Logical Rules
Jiacheng Li1∗, Haibo Din^2, Jingbo Shang1, Julian McAuley1, Zhe Feng2

1 University of California, San Diego

2 Bosch Research North America

**Abstract**

We study the problem of building entity tagging systems by using a few rules as weak supervision. Previous methods mostly focus on disambiguation entity types based on contexts and expert-provided rules, while assuming entity spans are given. In this work, we propose a novel method TALLOR that bootstraps high-quality logical rules to train a neural tagger in a fully automated manner. Specifically, we introduce compound rules that are composed from simple rules to increase the precision of boundary detection and generate more diverse pseudo labels. We further design a dynamic label selection strategy to ensure pseudo label quality and therefore avoid overfitting the neural tagger. Experiments on three datasets demonstrate that our method outperforms other weakly supervised methods and even rivals a state-of-the-art distantly supervised tagger with a lexicon of over 2,000 terms when starting from only 20 simple rules. Our method can serve as a tool for rapidly building taggers in emerging domains and tasks. Case studies show that learned rules can potentially explain the predicted entities.

[[paper]](Weakly Supervised Named Entity Tagging with Learnable Logical Rules) Submitted on 5 Jul 2021

#### 5.Leveraging Type Descriptions for Zero-shot Named Entity Recognition and Classification
Rami Aly1, Andreas Vlachos1, Ryan McDonald2∗

1 Computer Laboratory, University of Cambridge, U.K.

2 ASAPP

**Abstract**

A common issue in real-world applications of named entity recognition and classification (NERC) is the absence of annotated data for the target entity classes during training. Zero-shot learning approaches address this issue by learning models from classes with training data that can predict classes without it. This paper presents the first approach for zero-shot NERC, introducing novel architectures that leverage the fact that textual descriptions for many entity classes occur naturally. We address the zero-shot NERC specific challenge that the not-an-entity class is not well defined as different entity classes are considered in training and testing. For evaluation, we adapt two datasets, OntoNotes and MedMentions, emulating the difficulty of real-world zero-shot learning by testing models on the rarest entity classes. Our proposed approach outperforms baselines adapted from machine reading comprehension and zero-shot text classification. Furthermore, we assess the effect of different class descriptions for this task.

[[paper]](https://aclanthology.org/2021.acl-long.120/)

#### 6. Learning from Miscellaneous Other-Class Words for Few-shot Named Entity Recognition
Meihan Tong1, Shuai Wang2, Bin Xu1∗, Yixin Cao3, Minghui Liu1, Lei Hou1, Juanzi Li1

1 Knowledge Engineering Laboratory, Tsinghua University, Beijing, China

2 SLP Group, AI Technology Department, JOYY Inc, China

3 S-Lab Nanyang Technological University, Singapore

**Abstract**

Few-shot Named Entity Recognition (NER) exploits only a handful of annotations to identify and classify named entity mentions. Prototypical network shows superior performance on few-shot NER. However, existing prototypical methods fail to differentiate rich semantics in other-class words, which will aggravate overfitting under few shot scenario. To address the issue, we propose a novel model, Mining Undefined Classes from Other-class (MUCO), that can automatically induce different undefined classes from the other class to improve few-shot NER. With these extra-labeled undefined classes, our method will improve the discriminative ability of NER classifier and enhance the understanding of predefined classes with stand-by semantic knowledge. Experimental results demonstrate that our model outperforms five state-of-the-art models in both 1-shot and 5-shots settings on four NER benchmarks. We will release the code upon acceptance. The source code is released on https: //github.com/shuaiwa16/OtherClassNER.git.

[[paper]](https://arxiv.org/abs/2106.15167) Submitted on 29 Jun 2021

### 中文&多模NER

#### 1. MECT: Multi-Metadata Embedding based Cross-Transformer for Chinese Named Entity Recognition
Shuang Wu1, Xiaoning Song1∗, Zhenhua Feng2,3∗

1 School of Artificial Intelligence and Computer Science, Jiangnan University, China

2 Department of Computer Science, University of Surrey, UK

3 Centre for Vision, Speech and Signal Processing, University of Surrey, UK 

**Abstract**

Recently, word enhancement has become very popular for Chinese Named Entity Recognition (NER), reducing segmentation errors and increasing the semantic and boundary information of Chinese words. However, these methods tend to ignore the information of the Chinese character structure after integrating the lexical information. Chinese characters have evolved from pictographs since ancient times, and their structure often reflects more information about the characters. This paper presents a novel Multi-metadata Embedding based Cross-Transformer (MECT) to improve the performance of Chinese NER by fusing the structural information of Chinese characters. Specifically, we use multi-metadata embedding in a two-stream Transformer to integrate Chinese character features with the radical-level embedding. With the structural characteristics of Chinese characters, MECT can better capture the semantic information of Chinese characters for NER. The experimental results obtained on several well-known benchmarking datasets demonstrate the merits and superiority of the proposed MECT method.\footnote{The source code of the proposed method is publicly available at [this https URL](https://github.com/CoderMusou/MECT4CNER).

[[paper]](https://arxiv.org/abs/2107.05418v1) Submitted on 12 Jul 2021

#### 2. A Large-Scale Chinese Multimodal NER Dataset with Speech Clues
Dianbo Sui, Zhengkun Tian, Yubo Chen, Kang Liu, Jun Zhao

National Laboratory of Pattern Recognition, Institute of Automation,  CAS School of Artificial Intelligence, University of Chinese Academy of Sciences 

**Abstract**

In this paper, we aim to explore an uncharted territory, which is Chinese multimodal named entity recognition (NER) with both textual and acoustic contents. To achieve this, we construct a large-scale human-annotated Chinese multimodal NER dataset, named CNERTA. Our corpus totally contains 42,987 annotated sentences accompanying by 71 hours of speech data. Based on this dataset, we propose a family of strong and representative baseline models, which can leverage textual features or multimodal features. Upon these baselines, to capture the natural monotonic alignment between the textual modality and the acoustic modality, we further propose a simple multimodal multitask model by introducing a speech-to-text alignment auxiliary task. Through extensive experiments, we observe that: (1) Progressive performance boosts as we move from unimodal to multimodal, verifying the necessity of integrating speech clues into Chinese NER. (2) Our proposed model yields state-of-the-art (SoTA) results on CNERTA, demonstrating its effectiveness. For further research, the annotated dataset is publicly available at http://github.com/DianboWork/CNERTA.

[[paper]](https://aclanthology.org/2021.acl-long.218.pdf)

### 实体标准化

#### 1. An End-to-End Progressive Multi-Task Learning Framework for Medical Named Entity Recognition and Normalization
Baohang Zhou1,3, Xiangrui Cai2,3, Ying Zhang1,3,∗, Xiaojie Yuan1,3

1 College of Computer Science, Nankai University, Tianjin 300350, China

2 College of Cyber Science, Nankai University, Tianjin 300350, China

3 Tianjin Key Laboratory of Network and Data Security Technology, Tianjin 300350, China

**Abstract**

Medical named entity recognition (NER) and normalization (NEN) are fundamental for constructing knowledge graphs and building QA systems. Existing implementations for medical NER and NEN are suffered from the error propagation between the two tasks. The mispredicted mentions from NER will directly influence the results of NEN. Therefore, the NER module is the bottleneck of the whole system. Besides, the learnable features for both tasks are beneficial to improving the model performance. To avoid the disadvantages of existing models and exploit the generalized representation across the two tasks, we design an end-to-end progressive multi-task learning model for jointly modeling medical NER and NEN in an effective way. There are three level tasks with progressive difficulty in the framework. The progressive tasks can reduce the error propagation with the incremental task settings which implies the lower level tasks gain the supervised signals other than errors from the higher level tasks to improve their performances. Besides, the context features are exploited to enrich the semantic information of entity mentions extracted by NER. The performance of NEN profits from the enhanced entity mention features. The standard entities from knowledge bases are introduced into the NER module for extracting corresponding entity mentions correctly. The empirical results on two publicly available medical literature datasets demonstrate the superiority of our method over nine typical methods.

[[paper]](https://aclanthology.org/2021.acl-long.485.pdf) 

#### 2. A Neural Transition-based Joint Model for Disease Named Entity Recognition and Normalization
Zongcheng Ji1, Tian Xia1, Mei Han1, Jing Xiao2

1 PAII Inc., Palo Alto, CA, USA

2 Ping An Technology, Shenzhen, China

**Abstract**

Disease is one of the fundamental entities in biomedical research. Recognizing such entities from biomedical text and then normalizing them to a standardized disease vocabulary offer a tremendous opportunity for many downstream applications. Previous studies have demonstrated that joint modeling of the two sub-tasks has superior performance than the pipelined counterpart. Although the neural joint model based on multi-task learning framework has achieved state-of-the-art performance, it suffers from the boundary inconsistency problem due to the separate decoding procedures. Moreover, it ignores the rich information (e.g., the text surface form) of each candidate concept in the vocabulary, which is quite essential for entity normalization. In this work, we propose a neural transition-based joint model to alleviate these two issues. We transform the end-to-end disease recognition and normalization task as an action sequence prediction task, which not only jointly learns the model with shared representations of the input, but also jointly searches the output by state transitions in one search space. Moreover, we introduce attention mechanisms to take advantage of the text surface form of each candidate concept for better normalization performance. Experimental results conducted on two publicly available datasets show the effectiveness of the proposed method.

[[paper]](https://aclanthology.org/2021.acl-long.219/)

### 实体分类

#### 1. Modeling Fine-Grained Entity Types with Box Embeddings
Yasumasa Onoe♠, Michael Boratko♢, Andrew McCallum♢♣, Greg Durrett♠

♠ The University of Texas at Austin

♢ University of Massachusetts Amherst

♣ Google Research

**Abstract**

Neural entity typing models typically represent fine-grained entity types as vectors in a high-dimensional space, but such spaces are not well-suited to modeling these types’ complex interdependencies. We study the ability of box embeddings, which embed concepts as d-dimensional hyperrectangles, to capture hierarchies of types even when these relationships are not defined explicitly in the ontology. Our model represents both types and entity mentions as boxes. Each mention and its context are fed into a BERT-based model to embed that mention in our box space; essentially, this model leverages typological clues present in the surface text to hypothesize a type representation for the mention. Box containment can then be used to derive both the posterior probability of a mention exhibiting a given type and the conditional probability relations between types themselves. We compare our approach with a vector-based typing model and observe state-of-the-art performance on several entity typing benchmarks. In addition to competitive typing performance, our box-based model shows better performance in prediction consistency (predicting a supertype and a subtype together) and confidence (i.e., calibration), demonstrating that the box-based model captures the latent type hierarchies better than the vector-based model does.

[[paper]](https://arxiv.org/pdf/2101.00345.pdf)

#### 2. Ultra-Fine Entity Typing with Weak Supervision from a Masked Language Model
Hongliang Dai1, Yangqiu Song1,Haixun Wang2

1 Department of CSE, HKUST

2 Instacart

**Abstract**

Recently, there is an effort to extend fine-grained entity typing by using a richer and ultra-fine set of types, and labeling noun phrases including pronouns and nominal nouns instead of just named entity mentions. A key challenge for this ultra-fine entity typing task is that human annotated data are extremely scarce, and the annotation ability of existing distant or weak supervision approaches is very limited. To remedy this problem, in this paper, we propose to obtain training data for ultra-fine entity typing by using a BERT Masked Language Model (MLM). Given a mention in a sentence, our approach constructs an input for the BERT MLM so that it predicts context dependent hypernyms of the mention, which can be used as type labels. Experimental results demonstrate that, with the help of these automatically generated labels, the performance of an ultra-fine entity typing model can be improved substantially. We also show that our approach can be applied to improve traditional fine-grained entity typing after performing simple type mapping.

[[paper]](https://arxiv.org/abs/2106.04098) Submitted on 8 Jun 2021

### 其他
#### 1. SpanNER: Named Entity Re-/Recognition as Span Prediction
Jinlan Fu∗1, Xuanjing Huang1, Pengfei Liu†2

1 Carnegie Mellon University

2 Fudan University

**Abstract**

Recent years have seen the paradigm shift of Named Entity Recognition (NER) systems from sequence labeling to span prediction. Despite its preliminary effectiveness, the span prediction model's architectural bias has not been fully understood. In this paper, we first investigate the strengths and weaknesses when the span prediction model is used for named entity recognition compared with the sequence labeling framework and how to further improve it, which motivates us to make complementary advantages of systems based on different paradigms. We then reveal that span prediction, simultaneously, can serve as a system combiner to re-recognize named entities from different systems' outputs. We experimentally implement 154 systems on 11 datasets, covering three languages, comprehensive results show the effectiveness of span prediction models that both serve as base NER systems and system combiners. We make all code and datasets available: [\url{this https URL}](https://github.com/neulab/spanner), as well as an online system demo: [\url{this http URL}](http://spanner.sh/). Our model also has been deployed into the ExplainaBoard platform, which allows users to flexibly perform a system combination of top-scoring systems in an interactive way: [\url{this http URL}](http://explainaboard.nlpedia.ai/leaderboard/task-ner/).

[[paper]](https://arxiv.org/abs/2106.00641v2)  last revised 6 Jun 2021

#### 2. Improving Named Entity Recognition by External Context Retrieving and Cooperative Learning
Xinyu Wang1,‡, Yong Jiang†∗, Nguyen Bach†, Tao Wang†, Zhongqiang Huang†, Fei Huang†, Kewei Tu1,∗
1 School of Information Science and Technology, Shanghai  Tech University

Shanghai Engineering Research Center of Intelligent Vision and Imaging

Shanghai Institute of Microsystem and Information Technology, Chinese Academy of Sciences

University of Chinese Academy of Sciences

†DAMO Academy, Alibaba Group 

**Abstract**

Recent advances in Named Entity Recognition (NER) show that document-level contexts can significantly improve model performance. In many application scenarios, however, such contexts are not available. In this paper, we propose to find external contexts of a sentence by retrieving and selecting a set of semantically relevant texts through a search engine, with the original sentence as the query. We find empirically that the contextual representations computed on the retrieval-based input view, constructed through the concatenation of a sentence and its external contexts, can achieve significantly improved performance compared to the original input view based only on the sentence. Furthermore, we can improve the model performance of both input views by Cooperative Learning, a training method that encourages the two input views to produce similar contextual representations or output label distributions. Experiments show that our approach can achieve new state-of-the-art performance on 8 NER data sets across 5 domains.

[[paper]](https://arxiv.org/abs/2105.03654) last revised 2 Jun 2021 

#### 3. Modularized Interaction Network for Named Entity Recognition
Fei Li1, Zheng Wang2∗, Siu Cheung Hui2, Lejian Liao1, Dandan Song1∗, Jing Xu1, Guoxiu He3, Meihuizi Jia1

1 Beijing Institute of Technology, China

2 Nanyang Technological University, Singapore

3 Wuhan University, China

**Abstract**

Although the existing Named Entity Recognition(NER) models have achieved promising performance, they suffer from certain draw-backs. The sequence labeling-based NER models do not perform well in recognizing long entities as they focus only on word-level information, while the segment-based NER models which focus on processing segment instead of single word are unable to capture the word-level dependencies within the segment. Moreover, as boundary detection and type prediction may cooperate with each other for the NER task, it is also important for the two sub-tasks to mutually reinforce each other by sharing their information. In this paper, we propose a novel Modularized Interaction Network(MIN) model which utilizes both segment-level information and word-level dependencies, and incorporates an interaction mechanism to support information sharing between boundary detection and type prediction to enhance the performance for the NER task. We have conducted extensive experiments based on three NER benchmark datasets. The performance results have shown that the proposed MIN model has outperformed the current state-of-the-art models.

[[paper]](https://personal.ntu.edu.sg/wang_zheng/paper/ACL2021.pdf) 

#### 4. BERTifying the Hidden Markov Model for Multi-Source Weakly Supervised Named Entity Recognition
Yinghao Li1, Pranav Shetty1, Lucas Liu1, Chao Zhang1, and Le Song2

1 Georgia Institute of Technology, Atlanta, USA

2 Mohamed bin Zayed University of Artificial Intelligence, Abu Dhabi, United Arab Emirates 

**Abstract**

We study the problem of learning a named entity recognition (NER) model using noisy la-bels from multiple weak supervision sources. Though cheaper than human annotators, weak sources usually yield incomplete, inaccurate, or contradictory predictions. To address such challenges, we propose a conditional hidden Markov model (CHMM). It inherits the hidden Markov model's ability to aggregating the labels from weak sources through unsupervised learning. However, CHMM enhances the hidden Markov model's flexibility and context representation capability by predicting token-wise transition and emission probabilities from the BERT embeddings of the input tokens. In addition, we refine CHMM's prediction with an alternate-training approach (CHMM-AlT). It fine-tunes a BERT-based NER model with the labels inferred by CHMM, and this BERT-NER's output is regarded as an additional weak source to train the CHMM in return. Evaluation on four datasets from various domains shows that our method is superior to the weakly super-vised baselines by a wide margin.

[[paper]](https://arxiv.org/abs/2105.12848v2) last revised 30 May 2021 

#### 5. De-biasing Distantly Supervised Named Entity Recognition via Causal Intervention
Wenkai Zhang1,3, Hongyu Lin1,∗, Xianpei Han1,2,∗, Le Sun1,2

1 Chinese Information Processing Laboratory 

2 State Key Laboratory of Computer Science Institute of Software, Chinese Academy of Sciences, Beijing, China

3 University of Chinese Academy of Sciences, Beijing, China

**Abstract**

Distant supervision tackles the data bottleneck in NER by automatically generating training instances via dictionary matching. Unfortunately, the learning of DS-NER is severely dictionary-biased, which suffers from spurious correlations and therefore undermines the effectiveness and the robustness of the learned models. In this paper, we fundamentally explain the dictionary bias via a Structural Causal Model (SCM), categorize the bias into intra-dictionary and inter-dictionary biases, and identify their causes. Based on the SCM, we learn de-biased DS-NER via causal interventions. For intra-dictionary bias, we conduct backdoor adjustment to remove the spurious correlations introduced by the dictionary confounder. For inter-dictionary bias, we propose a causal invariance regularizer which will make DS-NER models more robust to the perturbation of dictionaries. Experiments on four datasets and three DS-NER models show that our method can significantly improve the performance of DS-NER.

[[paper]](https://arxiv.org/abs/2106.09233) Submitted on 17 Jun 2021

#### 6. Crowdsourcing Learning as Domain Adaptation: A Case Study on Named Entity Recognition
Xin Zhang1, Guangwei Xu, Yueheng Sun2, Meishan Zhang1∗, Pengjun Xie

1 School of New Media and Communication, Tianjin University, China

2 College of Intelligence and Computing, Tianjin University, China

**Abstract**

Crowdsourcing is regarded as one prospective solution for effective supervised learning, aiming to build large-scale annotated training data by crowd workers. Previous studies focus on reducing the influences from the noises of the crowdsourced annotations for supervised models. We take a different point in this work, regarding all crowdsourced annotations as gold-standard with respect to the individual annotators. In this way, we find that crowdsourcing could be highly similar to domain adaptation, and then the recent advances of cross-domain methods can be almost directly applied to crowdsourcing. Here we take named entity recognition (NER) as a study case, suggesting an annotator-aware representation learning model that inspired by the domain adaptation methods which attempt to capture effective domain-aware features. We investigate both unsupervised and supervised crowdsourcing learning, assuming that no or only small-scale expert annotations are available. Experimental results on a benchmark crowdsourced NER dataset show that our method is highly effective, leading to a new state-of-the-art performance. In addition, under the supervised setting, we can achieve impressive performance gains with only a very small scale of expert annotations.

[[paper]](https://arxiv.org/abs/2105.14980v1) Submitted on 31 May 2021

#### 7. LNN-EL: A Neuro-Symbolic Approach to Short-text Entity Linking
Hang Jiang1∗, Sairam Gurajada2∗, Qiuhao Lu3, Sumit Neelam2, Lucian Popa2, Prithviraj Sen2, Yunyao Li2, Alexander Gray2

1 MIT 

2 IBM Research 

3 University of Oregon

**Abstract**

Entity linking (EL), the task of disambiguating mentions in text by linking them to entities in a knowledge graph, is crucial for text understanding, question answering or conversational systems. Entity linking on short text (e.g., single sentence or question) poses particular challenges due to limited context. While prior approaches use either heuristics or black-box neural methods, here we propose LNN-EL, a neuro-symbolic approach that combines the advantages of using interpretable rules based on first-order logic with the performance of neural learning. Even though constrained to using rules, LNN-EL performs competitively against SotA black-box neural approaches, with the added benefits of extensibility and transferability. In particular, we show that we can easily blend existing rule templates given by a human expert, with multiple types of features (priors, BERT encodings, box embeddings, etc), and even scores resulting from previous EL methods, thus improving on such methods. For instance, on the LC-QuAD-1.0 dataset, we show more than 4\% increase in F1 score over previous SotA. Finally, we show that the inductive bias offered by using logic results in learned rules that transfer well across datasets, even without fine tuning, while maintaining high accuracy.

[[paper]](https://arxiv.org/abs/2106.09795) Submitted on 17 Jun 2021

## 二、关系抽取
### 远程监督

#### 1. CIL: Contrastive Instance Learning Framework for Distantly Supervised Relation Extraction
Tao Chen1,2, Haizhou Shi1, Siliang Tang1,2∗, Zhigang Chen3,Fei Wu1 & Yueting Zhuang1

1 Zhejiang University

2 Alibaba-Zhejiang University Joint Research Institute of Frontier Technologies

3 State Key Laboratory of Cognitive Intelligence, Hefei, China

**Abstract**

The journey of reducing noise from distant supervision (DS) generated training data has been started since the DS was first introduced into the relation extraction (RE) task. For the past decade, researchers apply the multi-instance learning (MIL) framework to find the most reliable feature from a bag of sentences. Although the pattern of MIL bags can greatly reduce DS noise, it fails to represent many other useful sentence features in the datasets. In many cases, these sentence features can only be acquired by extra sentence-level human annotation with heavy costs. Therefore, the performance of distantly supervised RE models is bounded. In this paper, we go beyond typical MIL framework and propose a novel contrastive instance learning (CIL) framework. Specifically, we regard the initial MIL as the relational triple encoder and constraint positive pairs against negative pairs for each instance. Experiments demonstrate the effectiveness of our proposed framework, with significant improvements over the previous methods on NYT10, GDS and KBP.

[[paper]](https://arxiv.org/abs/2106.10855)Submitted on 21 Jun 2021

#### 2. How Knowledge Graph and Attention Help? A Qualitative Analysis into Bag-level Relation Extraction
Zikun Hu1, Yixin Cao2, Lifu Huang3, Tat-Seng Chua1

1 National University of Singapore

2 S-Lab, Nanyang Technological University

3 Computer Science Department, Virginia Tech

**Abstract** 

Knowledge Graph (KG) and attention mechanism have been demonstrated effective in introducing and selecting useful information for weakly supervised methods. However, only qualitative analysis and ablation study are provided as evidence. In this paper, we contribute a dataset and propose a paradigm to quantitatively evaluate the effect of attention and KG on bag-level relation extraction (RE). We find that (1) higher attention accuracy may lead to worse performance as it may harm the model's ability to extract entity mention features; (2) the performance of attention is largely influenced by various noise distribution patterns, which is closely related to real-world datasets; (3) KG-enhanced attention indeed improves RE performance, while not through enhanced attention but by incorporating entity prior; and (4) attention mechanism may exacerbate the issue of insufficient training data. Based on these findings, we show that a straightforward variant of RE model can achieve significant improvements (6% AUC on average) on two real-world datasets as compared with three state-of-the-art baselines. Our codes and datasets are available at [this https URL](https://github.com/zig-kwin-hu/how-KG-ATT-help).

[[paper]](https://arxiv.org/abs/2107.12064v1) Submitted on 26 Jul 2021

#### 2. SENT: Sentence-level Distant Relation Extraction via Negative Training
Ruotian Ma1, Tao Gui2∗, Linyang Li1, Qi Zhang1∗, Yaqian Zhou1, Xuanjing Huang1

1 School of Computer Science, Fudan University, Shanghai, China

2 Institute of Modern Languages and Linguistics, Fudan University, Shanghai, China

**Abstract**

Distant supervision for relation extraction provides uniform bag labels for each sentence inside the bag, while accurate sentence labels are important for downstream applications that need the exact relation type. Directly using bag labels for sentence-level training will introduce much noise, thus severely degrading performance. In this work, we propose the use of negative training (NT), in which a model is trained using complementary labels regarding that ``the instance does not belong to these complementary labels". Since the probability of selecting a true label as a complementary label is low, NT provides less noisy information. Furthermore, the model trained with NT is able to separate the noisy data from the training data. Based on NT, we propose a sentence-level framework, SENT, for distant relation extraction. SENT not only filters the noisy data to construct a cleaner dataset, but also performs a re-labeling process to transform the noisy data into useful training data, thus further benefiting the model's performance. Experimental results show the significant improvement of the proposed method over previous methods on sentence-level evaluation and de-noise effect.

[[paper]](https://arxiv.org/abs/2106.11566) Submitted on 22 Jun 2021

#### 3. Revisiting the Negative Data of Distantly Supervised Relation Extraction
Chenhao Xie, Jiaqing Liang, Jingping Liu, Chengsong Huang, Wenhao Huang, Yanghua Xiao

Fudan University, Shanghai, China

**Abstract**

Distantly supervision automatically generates plenty of training samples for relation extraction. However, it also incurs two major problems: noisy labels and imbalanced training data. Previous works focus more on reducing wrongly labeled relations (false positives) while few explore the missing relations that are caused by incompleteness of knowledge base (false negatives). Furthermore, the quantity of negative labels overwhelmingly surpasses the positive ones in previous problem formulations. In this paper, we first provide a thorough analysis of the above challenges caused by negative data. Next, we formulate the problem of relation extraction into as a positive unlabeled learning task to alleviate false negative problem. Thirdly, we propose a pipeline approach, dubbed \textsc{ReRe}, that performs sentence-level relation detection then subject/object extraction to achieve sample-efficient training. Experimental results show that the proposed method consistently outperforms existing approaches and remains excellent performance even learned with a large quantity of false positive samples.

[[paper]](https://arxiv.org/abs/2105.10158) Submitted on 21 May 2021

### 联合抽取

#### 1. Joint Biomedical Entity and Relation Extraction with Knowledge-Enhanced Collective Inference
Tuan Lai1, Heng Ji1, ChengXiang Zhai1, Quan Hung Tran2

1 University of Illinois at Urbana-Champaign 

2 Adobe Research

**Abstract**

Compared to the general news domain, information extraction (IE) from biomedical text requires much broader domain knowledge. However, many previous IE methods do not utilize any external knowledge during inference. Due to the exponential growth of biomedical publications, models that do not go beyond their fixed set of parameters will likely fall behind. Inspired by how humans look up relevant information to comprehend a scientific text, we present a novel framework that utilizes external knowledge for joint entity and relation extraction named KECI (Knowledge-Enhanced Collective Inference). Given an input text, KECI first constructs an initial span graph representing its initial understanding of the text. It then uses an entity linker to form a knowledge graph containing relevant background knowledge for the the entity mentions in the text. To make the final predictions, KECI fuses the initial span graph and the knowledge graph into a more refined graph using an attention mechanism. KECI takes a collective approach to link mention spans to entities by integrating global relational information into local representations using graph convolutional networks. Our experimental results show that the framework is highly effective, achieving new state-of-the-art results in two different benchmark datasets: BioRelEx (binding interaction detection) and ADE (adverse drug event extraction). For example, KECI achieves absolute improvements of 4.59% and 4.91% in F1 scores over the state-of-the-art on the BioRelEx entity and relation extraction tasks.

[[paper]](https://aclanthology.org/2021.acl-long.488/)

#### 2. UniRE: A Unified Label Space for Entity Relation Extraction
Yijun Wang∗1,2, Changzhi Sun∗4, Yuanbin Wu3, Hao Zhou4, Lei Li4, and Junchi Yan†1,2

1 Department of Computer Science and Engineering, Shanghai Jiao Tong University

2 MoE Key Lab of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University

3 School of Computer Science and Technology, East China Normal University

4 ByteDance AI Lab

**Abstract**

Many joint entity relation extraction models setup two separated label spaces for the two sub-tasks (i.e., entity detection and relation classification). We argue that this setting may hinder the information interaction between entities and relations. In this work, we propose to eliminate the different treatment on the two sub-tasks' label spaces. The input of our model is a table containing all word pairs from a sentence. Entities and relations are represented by squares and rectangles in the table. We apply a unified classifier to predict each cell's label, which unifies the learning of two sub-tasks. For testing, an effective (yet fast) approximate decoder is proposed for finding squares and rectangles from tables. Experiments on three benchmarks (ACE04, ACE05, SciERC) show that, using only half the number of parameters, our model achieves competitive accuracy with the best extractor, and is faster.

[[paper]](https://arxiv.org/abs/2107.04292v1) Submitted on 9 Jul 2021

#### 3. PRGC: Potential Relation and Global Correspondence Based Joint Relational Triple Extraction
Hengyi Zheng1,2,3, Rui Wen3, Xi Chen3,4∗, Yifan Yang3, Yunyan Zhang3, Ziheng Zhang3, Ningyu Zhang5, Bin Qin2∗, Ming Xu2∗, Yefeng Zheng3

1 College of Electronics and Information Engineering, Shenzhen University

2 Information Technology Center, Shenzhen University
3 Tencent Jarvis Lab, Shenzhen, China

4 Platform and Content Group, Tencent 5Zhejiang University

**Abstract**

Joint extraction of entities and relations from unstructured texts is a crucial task in information extraction. Recent methods achieve considerable performance but still suffer from some inherent limitations, such as redundancy of relation prediction, poor generalization of span-based extraction and inefficiency. In this paper, we decompose this task into three subtasks, Relation Judgement, Entity Extraction and Subject-object Alignment from a novel perspective and then propose a joint relational triple extraction framework based on Potential Relation and Global Correspondence (PRGC). Specifically, we design a component to predict potential relations, which constrains the following entity extraction to the predicted relation subset rather than all relations; then a relation-specific sequence tagging component is applied to handle the overlapping problem between subjects and objects; finally, a global correspondence component is designed to align the subject and object into a triple with low-complexity. Extensive experiments show that PRGC achieves state-of-the-art performance on public benchmarks with higher efficiency and delivers consistent performance gain on complex scenarios of overlapping triples.

[[paper]](https://arxiv.org/abs/2106.09895) Submitted on 18 Jun 2021

#### 4. Dependency-driven Relation Extraction with Attentive Graph Convolutional Networks
Yuanhe Tian1, Guimin Chen2, Yan Song3,4, Xiang Wan4

1 University of Washington 

2 QTrade

3 The Chinese University of Hong Kong (Shenzhen)

4 Shenzhen Research Institute of Big Data

**Abstract**

Syntactic information, especially dependency trees, has been widely used by existing studies to improve relation extraction with better semantic guidance for analyzing the context information associated with the given entities. However, most existing studies suffer from the noise in the dependency trees, especially when they are automatically generated, so that intensively leveraging dependency information may introduce confusions to relation classification and necessary pruning is of great importance in this task. In this paper, we propose a dependency-driven approach for relation extraction with attentive graph convolutional networks (A-GCN). In this approach, an attention mechanism upon graph convolutional networks is applied to different contextual words in the dependency tree obtained from an off-the-shelf dependency parser, to distinguish the importance of different word dependencies. Consider that dependency types among words also contain important contextual guidance, which is potentially helpful for relation extraction, we also include the type information in A-GCN modeling. Experimental results on two English benchmark datasets demonstrate the effectiveness of our A-GCN, which outperforms previous studies and achieves state-of-the-art performance on both datasets.

[[paper]](https://aclanthology.org/2021.acl-long.344/)

### 开放抽取
#### 1. CoRI: Collective Relation Integration with Data Augmentation for Open Information Extraction
Zhengbao Jiang1∗, Jialong Han2, Bunyamin Sisman2, Xin Luna Dong2

1 Language Technologies Institute, Carnegie Mellon University

2 Amazon

**Abstract**

Integrating extracted knowledge from the Web to knowledge graphs (KGs) can facilitate tasks like question answering. We study relation integration that aims to align free-text relations in subject-relation-object extractions to relations in a target KG. To address the challenge that free-text relations are ambiguous, previous methods exploit neighbor entities and relations for additional context. However, the predictions are made independently, which can be mutually inconsistent. We propose a two-stage Collective Relation Integration (CoRI) model, where the first stage independently makes candidate predictions, and the second stage employs a collective model that accesses all candidate predictions to make globally coherent predictions. We further improve the collective model with augmented data from the portion of the target KG that is otherwise unused. Experiment results on two datasets show that CoRI can significantly outperform the baselines, improving AUC from .677 to .748 and from .716 to .780, respectively.

[[paper]](https://arxiv.org/abs/2106.00793v1) Submitted on 1 Jun 2021

#### 2. Element Intervention for Open Relation Extraction
Fangchao Liu1,3, Lingyong Yan1,3, Hongyu Lin1,∗, Xianpei Han1,2,∗, Le Sun1,2

1 Chinese Information Processing Laboratory 

2 State Key Laboratory of Computer Science Institute of Software, Chinese Academy of Sciences, Beijing, China

3 University of Chinese Academy of Sciences, Beijing, China

**Abstract**

Open relation extraction aims to cluster relation instances referring to the same underlying relation, which is a critical step for general relation extraction. Current OpenRE models are commonly trained on the datasets generated from distant supervision, which often results in instability and makes the model easily collapsed. In this paper, we revisit the procedure of OpenRE from a causal view. By formulating OpenRE using a structural causal model, we identify that the above-mentioned problems stem from the spurious correlations from entities and context to the relation type. To address this issue, we conduct \emph{Element Intervention}, which intervenes on the context and entities respectively to obtain the underlying causal effects of them. We also provide two specific implementations of the interventions based on entity ranking and context contrasting. Experimental results on unsupervised relation extraction datasets show that our methods outperform previous state-of-the-art methods and are robust across different datasets.

[[paper]](Element Intervention for Open Relation Extraction) Submitted on 17 Jun 2021

### 事件关系抽取
#### 1. From Discourse to Narrative: Knowledge Projection for Event Relation Extraction
Jialong Tang1,3, Hongyu Lin1, Meng Liao4, Yaojie Lu1,3,Xianpei Han1,2, Le Sun1,2,∗, Weijian Xie4, Jin Xu4,∗

1 Chinese Information Processing Laboratory 

2 State Key Laboratory of Computer Science, Institute of Software, Chinese Academy of Sciences, Beijing, China

3 University of Chinese Academy of Sciences, Beijing, China

4 Data Quality Team, WeChat, Tencent Inc., China

**Abstract**

Current event-centric knowledge graphs highly rely on explicit connectives to mine relations between events. Unfortunately, due to the sparsity of connectives, these methods severely undermine the coverage of EventKGs. The lack of high-quality labelled corpora further exacerbates that problem. In this paper, we propose a knowledge projection paradigm for event relation extraction: projecting discourse knowledge to narratives by exploiting the commonalities between them. Specifically, we propose Multi-tier Knowledge Projection Network (MKPNet), which can leverage multi-tier discourse knowledge effectively for event relation extraction. In this way, the labelled data requirement is significantly reduced, and implicit event relations can be effectively extracted. Intrinsic experimental results show that MKPNet achieves the new state-of-the-art performance, and extrinsic experimental results verify the value of the extracted event relations.

[[paper]](https://arxiv.org/abs/2106.08629) Submitted on 16 Jun 2021

### 其他

#### 1. Refining Sample Embeddings with Relation Prototypes to Enhance Continual Relation Extraction
Li Cui1, Deqing Yang1∗, Jiaxin Yu1, Chengwei Hu1, Jiayang Cheng1,  Jingjie Yi1, Yanghua Xiao2,3∗

1 School of Data Science, Fudan University

2 Shanghai Key Laboratory of Data Science, School of Computer Science, Fudan University

3 Fudan-Aishu Cognitive Intelligence Joint Research Center, Shanghai, China

**Abstract**

Continual learning has gained increasing attention in recent years, thanks to its biological interpretation and efficiency in many real-world applications. As a typical task of continual learning, continual relation extraction (CRE) aims to extract relations between entities from texts, where the samples of different relations are delivered into the model continuously. Some previous works have proved that storing typical samples of old relations in memory can help the model keep a stable understanding of old relations and avoid forgetting them. However, most methods heavily depend on the memory size in that they simply replay these memorized samples in subsequent tasks. To fully utilize memorized samples, in this paper, we employ relation prototype to extract useful information of each relation. Specifically, the prototype embedding for a specific relation is computed based on memorized samples of this relation, which is collected by K-means algorithm. The prototypes of all observed relations at current learning stage are used to re-initialize a memory network to refine subsequent sample embeddings, which ensures the model’s stable understanding on all observed relations when learning a new task. Compared with previous CRE models, our model utilizes the memory information sufficiently and efficiently, resulting in enhanced CRE performance. Our experiments show that the proposed model outperforms the state-of-the-art CRE models and has great advantage in avoiding catastrophic forgetting. The code and datasets are released on https://github.com/fd2014cl/RP-CRE.

[[paper]](https://aclanthology.org/2021.acl-long.20/)

### 三、事件抽取
#### 1. Capturing Event Argument Interaction via A Bi-Directional Entity-Level Recurrent Decoder
Xi Xiangyu1,2, Wei Ye1,†, Shikun Zhang1,†, Quanxiu Wang3, Huixing Jiang2, Wei Wu2

1 National Engineering Research Center for Software 
Engineering, Peking University,Beijing, China

2 Meituan Group, Beijing, China

3 RICH AI, Beijing, China

**Abstract**

Capturing interactions among event arguments is an essential step towards robust event argument extraction (EAE). However, existing efforts in this direction suffer from two limitations: 1) The argument role type information of contextual entities is mainly utilized as training signals, ignoring the potential merits of directly adopting it as semantically rich input features; 2) The argument-level sequential semantics, which implies the overall distribution pattern of argument roles over an event mention, is not well characterized. To tackle the above two bottlenecks, we formalize EAE as a Seq2Seq-like learning problem for the first time, where a sentence with a specific event trigger is mapped to a sequence of event argument roles. A neural architecture with a novel Bi-directional Entity-level Recurrent Decoder (BERD) is proposed to generate argument roles by incorporating contextual entities' argument role predictions, like a word-by-word text generation process, thereby distinguishing implicit argument distribution patterns within an event more accurately.

[[paper]](https://arxiv.org/abs/2107.00189v1) Submitted on 1 Jul 2021

#### 2. Verb Knowledge Injection for Multilingual Event Processing
Olga Majewska1, Ivan Vulic1, Goran Glavaš2, Edoardo M. Ponti1,3, Anna Korhonen1

1 Language Technology Lab, TAL, University of Cambridge, UK

2 Data and Web Science Group, University of Mannheim, Germany

3 Mila – Quebec AI Institute, Montreal, Canada

**Abstract**

In parallel to their overwhelming success across NLP tasks, language ability of deep Transformer networks, pretrained via language modeling (LM) objectives has undergone extensive scrutiny. While probing revealed that these models encode a range of syntactic and semantic properties of a language, they are still prone to fall back on superficial cues and simple heuristics to solve downstream tasks, rather than leverage deeper linguistic knowledge. In this paper, we target one such area of their deficiency, verbal reasoning. We investigate whether injecting explicit information on verbs' semantic-syntactic behaviour improves the performance of LM-pretrained Transformers in event extraction tasks -- downstream tasks for which accurate verb processing is paramount. Concretely, we impart the verb knowledge from curated lexical resources into dedicated adapter modules (dubbed verb adapters), allowing it to complement, in downstream tasks, the language knowledge obtained during LM-pretraining. We first demonstrate that injecting verb knowledge leads to performance gains in English event extraction. We then explore the utility of verb adapters for event extraction in other languages: we investigate (1) zero-shot language transfer with multilingual Transformers as well as (2) transfer via (noisy automatic) translation of English verb-based lexical constraints. Our results show that the benefits of verb knowledge injection indeed extend to other languages, even when verb adapters are trained on noisily translated constraints.

[[paper]](https://arxiv.org/abs/2012.15421v1) Submitted on 31 Dec 2020

#### 3. OntoED: Low-resource Event Detection with Ontology Embedding
Shumin Deng1∗, Ningyu Zhang1∗, Luoqiu Li, Hui Chen3, Huaixiao Tou3, Mosha Chen3, Fei Huang3, Huajun Chen1,2†

1 Zhejiang University & AZFT Joint Lab for Knowledge Engine

2 Hangzhou Innovation Center, Zhejiang University

3 Alibaba Group, China

**Abstract**

Event Detection (ED) aims to identify event trigger words from a given text and classify it into an event type. Most of current methods to ED rely heavily on training instances, and almost ignore the correlation of event types. Hence, they tend to suffer from data scarcity and fail to handle new unseen event types. To address these problems, we formulate ED as a process of event ontology population: linking event instances to pre-defined event types in event ontology, and propose a novel ED framework entitled OntoED with ontology embedding. We enrich event ontology with linkages among event types, and further induce more event-event correlations. Based on the event ontology, OntoED can leverage and propagate correlation knowledge, particularly from data-rich to data-poor event types. Furthermore, OntoED can be applied to new unseen event types, by establishing linkages to existing ones. Experiments indicate that OntoED is more predominant and robust than previous approaches to ED, especially in data-scarce scenarios.

[[paper]](https://arxiv.org/abs/2105.10922v3) last revised 27 May 2021

#### 4. Document-level Event Extraction via Heterogeneous Graph-based Interaction Model with a Tracker
Runxin Xu1, Tianyu Liu1, Lei Li3, Baobao Chang1,2

1 Key Laboratory of Computational Linguistics, Peking 
University, MOE, China

2 Peng Cheng Laboratory, Shenzhen, China

3 ByteDance AI Lab

**Abstract**

Document-level event extraction aims to recognize event information from a whole piece of article. Existing methods are not effective due to two challenges of this task: a) the target event arguments are scattered across sentences; b) the correlation among events in a document is non-trivial to model. In this paper, we propose Heterogeneous Graph-based Interaction Model with a Tracker (GIT) to solve the aforementioned two challenges. For the first challenge, GIT constructs a heterogeneous graph interaction network to capture global interactions among different sentences and entity mentions. For the second, GIT introduces a Tracker module to track the extracted events and hence capture the interdependency among the events. Experiments on a large-scale dataset (Zheng et al., 2019) show GIT outperforms the previous methods by 2.8 F1. Further analysis reveals GIT is effective in extracting multiple correlated events and event arguments that scatter across the document. Our code is available at this https URL.

[[paper]](https://arxiv.org/abs/2105.14924) Submitted on 31 May 2021

#### 5. LearnDA: Learnable Knowledge-Guided Data Augmentation for Event Causality Identification
Xinyu Zuo1,2, Pengfei Cao1,2, Yubo Chen1,2, Kang Liu1,2, Jun Zhao1,2, Weihua Peng3, Yuguang Chen3

1 National Laboratory of Pattern Recognition, Institute of Automation, CAS, Beijing, China

2 School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China

3 Beijing Baidu Netcom Science Technology Co., Ltd

**Abstract**

Modern models for event causality identification (ECI) are mainly based on supervised learning, which are prone to the data lacking problem. Unfortunately, the existing NLP-related augmentation methods cannot directly produce the available data required for this task. To solve the data lacking problem, we introduce a new approach to augment training data for event causality identification, by iteratively generating new examples and classifying event causality in a dual learning framework. On the one hand, our approach is knowledge-guided, which can leverage existing knowledge bases to generate well-formed new sentences. On the other hand, our approach employs a dual mechanism, which is a learnable augmentation framework and can interactively adjust the generation process to generate task-related sentences. Experimental results on two benchmarks EventStoryLine and Causal-TimeBank show that 1) our method can augment suitable task-related training data for ECI; 2) our method outperforms previous methods on EventStoryLine and Causal-TimeBank (+2.5 and +2.1 points on F1 value respectively).

[[paper]](https://arxiv.org/abs/2106.01649) Submitted on 3 Jun 2021

#### 6. MLBiNet: A Cross-Sentence Collective Event Detection Network
Dongfang Lou1,2∗, Zhilin Liao1,2∗, Shumin Deng1,2, Ningyu Zhang1,2†,  Huajun Chen1,2†

1 Zhejiang University & AZFT Joint Lab for Knowledge Engine

2 Hangzhou Innovation Center, Zhejiang University

**Abstract**

We consider the problem of collectively detecting multiple events, particularly in cross-sentence settings. The key to dealing with the problem is to encode semantic information and model event inter-dependency at a document-level. In this paper, we reformulate it as a Seq2Seq task and propose a Multi-Layer Bidirectional Network (MLBiNet) to capture the document-level association of events and semantic information simultaneously. Specifically, a bidirectional decoder is firstly devised to model event inter-dependency within a sentence when decoding the event tag vector sequence. Secondly, an information aggregation module is employed to aggregate sentence-level semantic and event tag information. Finally, we stack multiple bidirectional decoders and feed cross-sentence information, forming a multi-layer bidirectional tagging architecture to iteratively propagate information across sentences. We show that our approach provides significant improvement in performance compared to the current state-of-the-art results.

[[paper]](https://arxiv.org/abs/2105.09458v3) last revised 1 Jun 2021

#### 7. Unleash GPT-2 Power for Event Detection
Amir Pouran Ben Veyseh1, Viet Dac Lai1, Franck Dernoncourt2, , Thien Huu Nguyen1

1 Department of Computer and Information Science, University of Oregon, Eugene, OR 97403, USA

2 Adobe Research, San Jose, CA, USA

**Abstract**

Event Detection (ED) aims to recognize mentions of events (i.e., event triggers) and their types in text. Recently, several ED datasets in various domains have been proposed. However, the major limitation of these resources is the lack of enough training data for individual event types which hinders the efficient training of data-hungry deep learning models. To overcome this issue, we propose to exploit the powerful pre-trained language model GPT-2 to generate training samples for ED. To prevent the noises inevitable in automatically generated data from hampering training process, we propose to exploit a teacher-student architecture in which the teacher is supposed to learn anchor knowledge from the original data. The student is then trained on combination of the original and GPT-generated data while being led by the anchor knowledge from the teacher. Optimal transport is introduced to facilitate the anchor knowledge-based guidance between the two networks. We evaluate the proposed model on multiple ED benchmark datasets, gaining consistent improvement and establishing state-of-the-art results for ED.

[[paper]](https://aclanthology.org/2021.acl-long.490.pdf)

#### 8. Document-Level Event Argument Extraction via Optimal Transport
Sha Li, Heng Ji, Jiawei Han

University of Illinois at Urbana-Champaign, IL, USA

**Abstract**

Event Argument Extraction (EAE) is one of the sub-tasks of event extraction, aiming to recognize the role of each entity mention toward a specific event trigger. Despite the success of prior works in sentence-level EAE, the document-level setting is less explored. In particular, whereas syntactic structures of sentences have been shown to be effective for sentence-level EAE, prior document-level EAE models totally ignore syntactic structures for documents. Hence, in this work, we study the importance of syntactic structures in document-level EAE. Specifically, we propose to employ Optimal Transport (OT) to induce structures of documents based on sentence-level syntactic structures and tailored to EAE task. Furthermore, we propose a novel regularization technique to explicitly constrain the contributions of unrelated context words in the final prediction for EAE. We perform extensive experiments on the benchmark document-level EAE dataset RAMS that leads to the state-of-the-art performance. Moreover, our experiments on the ACE 2005 dataset reveals the effectiveness of the proposed model in the sentence-level EAE by establishing new state-of-the-art results.

[[paper]](https://www.researchgate.net/publication/350853144_Document-Level_Event_Argument_Extraction_by_Conditional_Generation)

#### 9. Document-level Event Extraction via Parallel Prediction Networks
Kung-Hsiang Huang1, Nanyun Peng1,2

1 Information Sciences Institute, University of Southern California

2 Computer Science Department, University of California, Los Angeles

**Abstract**

Fully understanding narratives often requires identifying events in the context of whole documents and modeling the event relations. However, document-level event extraction is a challenging task as it requires the extraction of event and entity coreference, and capturing arguments that span across different sentences. Existing works on event extraction usually confine on extracting events from single sentences, which fail to capture the relationships between the event mentions at the scale of a document, as well as the event arguments that appear in a different sentence than the event trigger. In this paper, we propose an end-to-end model leveraging Deep Value Networks (DVN), a structured prediction algorithm, to efficiently capture cross-event dependencies for document-level event extraction. Experimental results show that our approach achieves comparable performance to CRF-based models on ACE05, while enjoys significantly higher computational efficiency.

[[paper]](https://arxiv.org/abs/2010.12787) last revised 7 May 2021

#### 10. Trigger is Not Sufficient: Exploiting Frame-aware Knowledge for Implicit Event Argument Extraction
Kaiwen Wei1,2, Sun Xian1,2, Zequn Zhang∗1,2, Jingyuan Zhang1,2, Zhi Guo1,2, Li Jin1,2

1 Key Laboratory of Network Information System Technology, Aerospace Information Research Institute, Chinese Academy of Sciences

2 School of Electronic, Electrical and Communication Engineering, University of Chinese Academy of Sciences

**Abstract**

Implicit Event Argument Extraction seeks to identify arguments that play direct or implicit roles in a given event. However, most prior works focus on capturing direct relations between arguments and the event trigger. The lack of reasoning ability brings many challenges to the extraction of implicit arguments. In this work, we present a Frame-aware Event Argument Extraction (FEAE) learning framework to tackle this issue through reasoning in event frame-level scope. The proposed method leverages related arguments of the expected one as clues to guide the reasoning process. To bridge the gap between oracle knowledge used in the training phase and the imperfect related arguments in the test stage, we further introduce a curriculum knowledge distillation strategy to drive a final model that could operate without extra inputs through mimicking the behavior of a well-informed teacher model. Experimental results demonstrate FEAE obtains new state-of-the-art performance on the RAMS dataset.

[[paper]](https://aclanthology.org/2021.acl-long.360.pdf)

#### 11. The Possible, the Plausible, and the Desirable: Event-Based Modality Detection for Language Processing
Valentina Pyatkin1∗, Shoval Sadde1∗, Aynat Rubinstein2

1 Bar Ilan University

2 Hebrew University of Jerusalem

**Abstract**

Modality is the linguistic ability to describe events with added information such as how desirable, plausible, or feasible they are. Modality is important for many NLP downstream tasks such as the detection of hedging, uncertainty, speculation, and more. Previous studies that address modality detection in NLP often restrict modal expressions to a closed syntactic class, and the modal sense labels are vastly different across different studies, lacking an accepted standard. Furthermore, these senses are often analyzed independently of the events that they modify. This work builds on the theoretical foundations of the Georgetown Gradable Modal Expressions (GME) work by Rubinstein et al. (2013) to propose an event-based modality detection task where modal expressions can be words of any syntactic class and sense labels are drawn from a comprehensive taxonomy which harmonizes the modal concepts contributed by the different studies. We present experiments on the GME corpus aiming to detect and classify fine-grained modal concepts and associate them with their modified events. We show that detecting and classifying modal expressions is not only feasible, but also improves the detection of modal events in their own right.

[[paper]](https://arxiv.org/abs/2106.08037v1) Submitted on 15 Jun 2021

#### 12. Text2Event: Controllable Sequence-to-Structure Generation for End-to-end Event Extraction
Yaojie Lu1,3, Hongyu Lin1, Jin Xu4,∗, Xianpei Han1,2,∗, Jialong Tang1,3, Annan Li1,3, Le Sun1,2, Meng Liao4, Shaoyi Chen4

1 Chinese Information Processing Laboratory

2 State Key Laboratory of Computer Science Institute of Software, Chinese Academy of Sciences, Beijing, China

3 University of Chinese Academy of Sciences, Beijing, China

4 Data Quality Team, WeChat, Tencent Inc., China

**Abstract**

Event extraction is challenging due to the complex structure of event records and the semantic gap between text and event. Traditional methods usually extract event records by decomposing the complex structure prediction task into multiple subtasks. In this paper, we propose Text2Event, a sequence-to-structure generation paradigm that can directly extract events from the text in an end-to-end manner. Specifically, we design a sequence-to-structure network for unified event extraction, a constrained decoding algorithm for event knowledge injection during inference, and a curriculum learning algorithm for efficient model learning. Experimental results show that, by uniformly modeling all tasks in a single model and universally predicting different labels, our method can achieve competitive performance using only record-level annotations in both supervised learning and transfer learning settings.

[[paper]](https://arxiv.org/abs/2106.09232) Submitted on 17 Jun 2021

### 四、信息抽取预训练

#### 1. ERICA: Improving Entity and Relation Understanding for Pre-trained Language Models via Contrastive Learning
Yujia Qin♣♠♦, Yankai Lin♦, Ryuichi Takanobu♣♦, Zhiyuan Liu♣∗, Peng Li^♦^, Heng Ji♠∗, Minlie Huang♣, Maosong Sun♣, Jie Zhou♦

♣ Department of Computer Science and Technology, Tsinghua University, Beijing, China

♠ University of Illinois at Urbana-Champaign

♦ Pattern Recognition Center, WeChat AI, Tencent Inc.

**Abstract**

Pre-trained Language Models (PLMs) have shown superior performance on various downstream Natural Language Processing (NLP) tasks. However, conventional pre-training objectives do not explicitly model relational facts in text, which are crucial for textual understanding. To address this issue, we propose a novel contrastive learning framework ERICA to obtain a deep understanding of the entities and their relations in text. Specifically, we define two novel pre-training tasks to better understand entities and relations: (1) the entity discrimination task to distinguish which tail entity can be inferred by the given head entity and relation; (2) the relation discrimination task to distinguish whether two relations are close or not semantically, which involves complex relational reasoning. Experimental results demonstrate that ERICA can improve typical PLMs (BERT and RoBERTa) on several language understanding tasks, including relation extraction, entity typing and question answering, especially under low-resource settings.

[[paper]](https://arxiv.org/abs/2012.15022v2)  last revised 26 May 2021 

#### 2.CLEVE: Contrastive Pre-training for Event Extraction
Ziqi Wang1∗, Xiaozhi Wang1∗, Xu Han1, Yankai Lin3, Lei Hou1,2†, Zhiyuan Liu1,2, Peng Li3, Juanzi Li1,2, Jie Zhou3

1 Department of Computer Science and Technology, BNRist

2 KIRC, Institute for Artificial Intelligence, Tsinghua University, Beijing, 100084, China

3 Pattern Recognition Center, WeChat AI, Tencent Inc, China

**Abstract**

Event extraction (EE) has considerably benefited from pre-trained language models (PLMs) by fine-tuning. However, existing pre-training methods have not involved modeling event characteristics, resulting in the developed EE models cannot take full advantage of large-scale unsupervised data. To this end, we propose CLEVE, a contrastive pre-training framework for EE to better learn event knowledge from large unsupervised data and their semantic structures (e.g. AMR) obtained with automatic parsers. CLEVE contains a text encoder to learn event semantics and a graph encoder to learn event structures respectively. Specifically, the text encoder learns event semantic representations by self-supervised contrastive learning to represent the words of the same events closer than those unrelated words; the graph encoder learns event structure representations by graph contrastive pre-training on parsed event-related semantic structures. The two complementary representations then work together to improve both the conventional supervised EE and the unsupervised "liberal" EE, which requires jointly extracting events and discovering event schemata without any annotated data. Experiments on ACE 2005 and MAVEN datasets show that CLEVE achieves significant improvements, especially in the challenging unsupervised setting. The source code and pre-trained checkpoints can be obtained from [this https URL](https://github.com/THU-KEG/CLEVE).

[[paper]](https://arxiv.org/abs/2105.14485) Submitted on 30 May 2021